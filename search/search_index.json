{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataclasses JSON This library provides a simple API for encoding and decoding dataclasses to and from JSON. It's very easy to get started. Documentation website Quickstart pip install dataclasses-json from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class SimpleExample : int_field : int simple_example = SimpleExample ( 1 ) # Encoding to JSON. Note the output is a string, not a dictionary. simple_example . to_json () # {\"int_field\": 1} # Encoding to a (JSON) dict simple_example . to_dict () # {'int_field': 1} # Decoding from JSON. Note the input is a string, not a dictionary. SimpleExample . from_json ( '{\"int_field\": 1}' ) # SimpleExample(1) # Decoding from a (JSON) dict SimpleExample . from_dict ({ 'int_field' : 1 }) # SimpleExample(1) What if you want to work with camelCase JSON? # same imports as above, with the additional `LetterCase` import from dataclasses import dataclass from dataclasses_json import dataclass_json , LetterCase @dataclass_json ( letter_case = LetterCase . CAMEL ) # now all fields are encoded/decoded from camelCase @dataclass class ConfiguredSimpleExample : int_field : int ConfiguredSimpleExample ( 1 ) . to_json () # {\"intField\": 1} ConfiguredSimpleExample . from_json ( '{\"intField\": 1}' ) # ConfiguredSimpleExample(1) Supported types It's recursive (see caveats below), so you can easily work with nested dataclasses. In addition to the supported types in the py to JSON table , this library supports the following: - any arbitrary Collection type is supported. Mapping types are encoded as JSON objects and str types as JSON strings. Any other Collection types are encoded into JSON arrays, but decoded into the original collection types. - datetime objects. datetime objects are encoded to float (JSON number) using timestamp . As specified in the datetime docs, if your datetime object is naive, it will assume your system local timezone when calling .timestamp() . JSON nunbers corresponding to a datetime field in your dataclass are decoded into a datetime-aware object, with tzinfo set to your system local timezone. Thus, if you encode a datetime-naive object, you will decode into a datetime-aware object. This is important, because encoding and decoding won't strictly be inverses. See this section if you want to override this default behavior (for example, if you want to use ISO). - UUID objects. They are encoded as str (JSON string). The latest release is compatible with both Python 3.7 and Python 3.6 (with the dataclasses backport). Usage Approach 1: Class decorator from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str lidatong = Person ( 'lidatong' ) # Encoding to JSON lidatong . to_json () # '{\"name\": \"lidatong\"}' # Decoding from JSON Person . from_json ( '{\"name\": \"lidatong\"}' ) # Person(name='lidatong') Note that the @dataclass_json decorator must be stacked above the @dataclass decorator (order matters!) Approach 2: Inherit from a mixin from dataclasses import dataclass from dataclasses_json import DataClassJsonMixin @dataclass class Person ( DataClassJsonMixin ): name : str lidatong = Person ( 'lidatong' ) # A different example from Approach 1 above, but usage is the exact same assert Person . from_json ( lidatong . to_json ()) == lidatong Pick whichever approach suits your taste. The differences in implementation are invisible in usage. How do I... Use my dataclass with JSON arrays or objects? from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str Encode into a JSON array containing instances of my Data Class people_json = [ Person ( 'lidatong' )] Person . schema () . dumps ( people_json , many = True ) # '[{\"name\": \"lidatong\"}]' Decode a JSON array containing instances of my Data Class people_json = '[{\"name\": \"lidatong\"}]' Person . schema () . loads ( people_json , many = True ) # [Person(name='lidatong')] Encode as part of a larger JSON object containing my Data Class (e.g. an HTTP request/response) import json person_dict = Person . schema () . dump ( Person ( 'lidatong' )) response_dict = { 'response' : { 'person' : person_dict } } response_json = json . dumps ( response_dict ) In this case, we do two steps. First, we encode the dataclass into a python dictionary rather than a JSON string, using schema() and dump . Scroll down for a section addressing that. Second, we leverage the built-in json.dumps to serialize our dataclass into a JSON string. Decode as part of a larger JSON object containing my Data Class (e.g. an HTTP response) import json response_dict = json . loads ( '{\"response\": {\"person\": {\"name\": \"lidatong\"}}}' ) person_dict = response_dict [ 'response' ] person = Person . schema () . load ( person_dict ) In a similar vein to encoding above, we leverage the built-in json module. First, call json.loads to read the entire JSON object into a dictionary. We then access the key of the value containing the encoded dict of our Person that we want to decode ( response_dict['response'] ). Second, we load in the dictionary using Person.schema().load . Encode or decode into Python lists/dictionaries rather than JSON? This can be by calling .schema() and then using the corresponding encoder/decoder methods, ie. .load(...) / .dump(...) . Encode into a single Python dictionary person = Person ( 'lidatong' ) person . to_dict () # {'name': 'lidatong'} Encode into a list of Python dictionaries people = [ Person ( 'lidatong' )] Person . schema () . dump ( people , many = True ) # [{'name': 'lidatong'}] Decode a dictionary into a single dataclass instance person_dict = { 'name' : 'lidatong' } Person . from_dict ( person_dict ) # Person(name='lidatong') Decode a list of dictionaries into a list of dataclass instances people_dicts = [{ \"name\" : \"lidatong\" }] Person . schema () . load ( people_dicts , many = True ) # [Person(name='lidatong')] Encode or decode from camelCase (or kebab-case)? JSON letter case by convention is camelCase, in Python members are by convention snake_case. from dataclasses import dataclass , field from dataclasses_json import LetterCase , dataclass_json @dataclass_json @dataclass class Person : given_name : str = field ( metadata = { 'dataclasses_json' : { 'letter_case' : LetterCase . CAMEL }} ) Person ( 'Alice' ) . to_json () # '{\"givenName\": \"Alice\"}' Person . from_json ( '{\"givenName\": \"Alice\"}' ) # Person('Alice') This library assumes your field follows the Python convention of snake_case naming. If your field is not snake_case to begin with and you attempt to parameterize LetterCase , the behavior of encoding/decoding is undefined (most likely it will result in subtle bugs). Handle missing or optional field values when decoding? By default, any fields in your dataclass that use default or default_factory will have the values filled with the provided default, if the corresponding field is missing from the JSON you're decoding. Decode JSON with missing field @dataclass_json @dataclass class Student : id : int name : str = 'student' Student . from_json ( '{\"id\": 1}' ) # Student(id=1, name='student') Notice from_json filled the field name with the specified default 'student' when it was missing from the JSON. Sometimes you have fields that are typed as Optional , but you don't necessarily want to assign a default. In that case, you can use the infer_missing kwarg to make from_json infer the missing field value as None . Decode optional field without default @dataclass_json @dataclass class Tutor : id : int student : Optional [ Student ] = None Tutor . from_json ( '{\"id\": 1}' ) # Tutor(id=1, student=None) Personally I recommend you leverage dataclass defaults rather than using infer_missing , but if for some reason you need to decouple the behavior of JSON decoding from the field's default value, this will allow you to do so. Explanation Briefly, on what's going on under the hood in the above examples: calling .schema() will have this library generate a marshmallow schema for you. It also fills in the corresponding object hook, so that marshmallow will create an instance of your Data Class on load (e.g. Person.schema().load returns a Person ) rather than a dict , which it does by default in marshmallow. Performance note .schema() is not cached (it generates the schema on every call), so if you have a nested Data Class you may want to save the result to a variable to avoid re-generation of the schema on every usage. person_schema = Person . schema () person_schema . dump ( people , many = True ) # later in the code... person_schema . dump ( person ) Override the default encode / decode / marshmallow field of a specific field? See Overriding Marshmallow interop Using the dataclass_json decorator or mixing in DataClassJsonMixin will provide you with an additional method .schema() . .schema() generates a schema exactly equivalent to manually creating a marshmallow schema for your dataclass. You can reference the marshmallow API docs to learn other ways you can use the schema returned by .schema() . You can pass in the exact same arguments to .schema() that you would when constructing a PersonSchema instance, e.g. .schema(many=True) , and they will get passed through to the marshmallow schema. from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str # You don't need to do this - it's generated for you by `.schema()`! from marshmallow import Schema , fields class PersonSchema ( Schema ): name = fields . Str () Overriding / Extending Overriding For example, you might want to encode/decode datetime objects using ISO format rather than the default timestamp . from dataclasses import dataclass , field from dataclasses_json import dataclass_json , config from datetime import datetime from marshmallow import fields @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : datetime = field ( metadata = config ( encoder = datetime . isoformat , decoder = datetime . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) ) ) Extending Similarly, you might want to extend dataclasses_json to encode date objects. from dataclasses import dataclass , field from dataclasses_json import dataclass_json , config from datetime import date from marshmallow import fields @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : date = field ( metadata = config ( encoder = date . isoformat , decoder = date . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) )) As you can see, you can override or extend the default codecs by providing a \"hook\" via a callable: - encoder : a callable, which will be invoked to convert the field value when encoding to JSON - decoder : a callable, which will be invoked to convert the JSON value when decoding from JSON - mm_field : a marshmallow field, which will affect the behavior of any operations involving .schema() Note that these hooks will be invoked regardless if you're using .to_json / dump / dumps and .from_json / load / loads . So apply overrides / extensions judiciously, making sure to carefully consider whether the interaction of the encode/decode/mm_field is consistent with what you expect! What if I have other dataclass field extensions that rely on metadata All the dataclasses_json.config does is return a mapping, namespaced under the key 'dataclasses_json' . Say there's another module, other_dataclass_package that uses metadata. Here's how you solve your problem: metadata = { 'other_dataclass_package' : 'some metadata...' } # pre-existing metadata for another dataclass package dataclass_json_config = config ( encoder = datetime . isoformat , decoder = datetime . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) ) metadata . update ( dataclass_json_config ) @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : datetime = field ( metadata = metadata ) You can also manually specify the dataclass_json configuration mapping. @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : date = field ( metadata = { 'dataclasses_json' : { 'encoder' : date . isoformat , 'decoder' : date . fromisoformat , 'mm_field' : fields . DateTime ( format = 'iso' ) }} ) A larger example from dataclasses import dataclass from dataclasses_json import dataclass_json from typing import List @dataclass_json @dataclass ( frozen = True ) class Minion : name : str @dataclass_json @dataclass ( frozen = True ) class Boss : minions : List [ Minion ] boss = Boss ([ Minion ( 'evil minion' ), Minion ( 'very evil minion' )]) boss_json = \"\"\" { \"minions\": [ { \"name\": \"evil minion\" }, { \"name\": \"very evil minion\" } ] } \"\"\" . strip () assert boss . to_json ( indent = 4 ) == boss_json assert Boss . from_json ( boss_json ) == boss Self Recursion Object hierarchies where fields are of the type that they are declared within require a small type hinting trick to declare the forward reference. from typing import Optional from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Tree (): value : str left : Optional [ 'Tree' ] right : Optional [ 'Tree' ] Avoid using from __future__ import annotations as it will cause problems with the way dataclasses_json accesses the type annotations.","title":"Home"},{"location":"#dataclasses-json","text":"This library provides a simple API for encoding and decoding dataclasses to and from JSON. It's very easy to get started. Documentation website","title":"Dataclasses JSON"},{"location":"#quickstart","text":"pip install dataclasses-json from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class SimpleExample : int_field : int simple_example = SimpleExample ( 1 ) # Encoding to JSON. Note the output is a string, not a dictionary. simple_example . to_json () # {\"int_field\": 1} # Encoding to a (JSON) dict simple_example . to_dict () # {'int_field': 1} # Decoding from JSON. Note the input is a string, not a dictionary. SimpleExample . from_json ( '{\"int_field\": 1}' ) # SimpleExample(1) # Decoding from a (JSON) dict SimpleExample . from_dict ({ 'int_field' : 1 }) # SimpleExample(1) What if you want to work with camelCase JSON? # same imports as above, with the additional `LetterCase` import from dataclasses import dataclass from dataclasses_json import dataclass_json , LetterCase @dataclass_json ( letter_case = LetterCase . CAMEL ) # now all fields are encoded/decoded from camelCase @dataclass class ConfiguredSimpleExample : int_field : int ConfiguredSimpleExample ( 1 ) . to_json () # {\"intField\": 1} ConfiguredSimpleExample . from_json ( '{\"intField\": 1}' ) # ConfiguredSimpleExample(1)","title":"Quickstart"},{"location":"#supported-types","text":"It's recursive (see caveats below), so you can easily work with nested dataclasses. In addition to the supported types in the py to JSON table , this library supports the following: - any arbitrary Collection type is supported. Mapping types are encoded as JSON objects and str types as JSON strings. Any other Collection types are encoded into JSON arrays, but decoded into the original collection types. - datetime objects. datetime objects are encoded to float (JSON number) using timestamp . As specified in the datetime docs, if your datetime object is naive, it will assume your system local timezone when calling .timestamp() . JSON nunbers corresponding to a datetime field in your dataclass are decoded into a datetime-aware object, with tzinfo set to your system local timezone. Thus, if you encode a datetime-naive object, you will decode into a datetime-aware object. This is important, because encoding and decoding won't strictly be inverses. See this section if you want to override this default behavior (for example, if you want to use ISO). - UUID objects. They are encoded as str (JSON string). The latest release is compatible with both Python 3.7 and Python 3.6 (with the dataclasses backport).","title":"Supported types"},{"location":"#usage","text":"","title":"Usage"},{"location":"#approach-1-class-decorator","text":"from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str lidatong = Person ( 'lidatong' ) # Encoding to JSON lidatong . to_json () # '{\"name\": \"lidatong\"}' # Decoding from JSON Person . from_json ( '{\"name\": \"lidatong\"}' ) # Person(name='lidatong') Note that the @dataclass_json decorator must be stacked above the @dataclass decorator (order matters!)","title":"Approach 1: Class decorator"},{"location":"#approach-2-inherit-from-a-mixin","text":"from dataclasses import dataclass from dataclasses_json import DataClassJsonMixin @dataclass class Person ( DataClassJsonMixin ): name : str lidatong = Person ( 'lidatong' ) # A different example from Approach 1 above, but usage is the exact same assert Person . from_json ( lidatong . to_json ()) == lidatong Pick whichever approach suits your taste. The differences in implementation are invisible in usage.","title":"Approach 2: Inherit from a mixin"},{"location":"#how-do-i","text":"","title":"How do I..."},{"location":"#use-my-dataclass-with-json-arrays-or-objects","text":"from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str Encode into a JSON array containing instances of my Data Class people_json = [ Person ( 'lidatong' )] Person . schema () . dumps ( people_json , many = True ) # '[{\"name\": \"lidatong\"}]' Decode a JSON array containing instances of my Data Class people_json = '[{\"name\": \"lidatong\"}]' Person . schema () . loads ( people_json , many = True ) # [Person(name='lidatong')] Encode as part of a larger JSON object containing my Data Class (e.g. an HTTP request/response) import json person_dict = Person . schema () . dump ( Person ( 'lidatong' )) response_dict = { 'response' : { 'person' : person_dict } } response_json = json . dumps ( response_dict ) In this case, we do two steps. First, we encode the dataclass into a python dictionary rather than a JSON string, using schema() and dump . Scroll down for a section addressing that. Second, we leverage the built-in json.dumps to serialize our dataclass into a JSON string. Decode as part of a larger JSON object containing my Data Class (e.g. an HTTP response) import json response_dict = json . loads ( '{\"response\": {\"person\": {\"name\": \"lidatong\"}}}' ) person_dict = response_dict [ 'response' ] person = Person . schema () . load ( person_dict ) In a similar vein to encoding above, we leverage the built-in json module. First, call json.loads to read the entire JSON object into a dictionary. We then access the key of the value containing the encoded dict of our Person that we want to decode ( response_dict['response'] ). Second, we load in the dictionary using Person.schema().load .","title":"Use my dataclass with JSON arrays or objects?"},{"location":"#encode-or-decode-into-python-listsdictionaries-rather-than-json","text":"This can be by calling .schema() and then using the corresponding encoder/decoder methods, ie. .load(...) / .dump(...) . Encode into a single Python dictionary person = Person ( 'lidatong' ) person . to_dict () # {'name': 'lidatong'} Encode into a list of Python dictionaries people = [ Person ( 'lidatong' )] Person . schema () . dump ( people , many = True ) # [{'name': 'lidatong'}] Decode a dictionary into a single dataclass instance person_dict = { 'name' : 'lidatong' } Person . from_dict ( person_dict ) # Person(name='lidatong') Decode a list of dictionaries into a list of dataclass instances people_dicts = [{ \"name\" : \"lidatong\" }] Person . schema () . load ( people_dicts , many = True ) # [Person(name='lidatong')]","title":"Encode or decode into Python lists/dictionaries rather than JSON?"},{"location":"#encode-or-decode-from-camelcase-or-kebab-case","text":"JSON letter case by convention is camelCase, in Python members are by convention snake_case. from dataclasses import dataclass , field from dataclasses_json import LetterCase , dataclass_json @dataclass_json @dataclass class Person : given_name : str = field ( metadata = { 'dataclasses_json' : { 'letter_case' : LetterCase . CAMEL }} ) Person ( 'Alice' ) . to_json () # '{\"givenName\": \"Alice\"}' Person . from_json ( '{\"givenName\": \"Alice\"}' ) # Person('Alice') This library assumes your field follows the Python convention of snake_case naming. If your field is not snake_case to begin with and you attempt to parameterize LetterCase , the behavior of encoding/decoding is undefined (most likely it will result in subtle bugs).","title":"Encode or decode from camelCase (or kebab-case)?"},{"location":"#handle-missing-or-optional-field-values-when-decoding","text":"By default, any fields in your dataclass that use default or default_factory will have the values filled with the provided default, if the corresponding field is missing from the JSON you're decoding. Decode JSON with missing field @dataclass_json @dataclass class Student : id : int name : str = 'student' Student . from_json ( '{\"id\": 1}' ) # Student(id=1, name='student') Notice from_json filled the field name with the specified default 'student' when it was missing from the JSON. Sometimes you have fields that are typed as Optional , but you don't necessarily want to assign a default. In that case, you can use the infer_missing kwarg to make from_json infer the missing field value as None . Decode optional field without default @dataclass_json @dataclass class Tutor : id : int student : Optional [ Student ] = None Tutor . from_json ( '{\"id\": 1}' ) # Tutor(id=1, student=None) Personally I recommend you leverage dataclass defaults rather than using infer_missing , but if for some reason you need to decouple the behavior of JSON decoding from the field's default value, this will allow you to do so.","title":"Handle missing or optional field values when decoding?"},{"location":"#explanation","text":"Briefly, on what's going on under the hood in the above examples: calling .schema() will have this library generate a marshmallow schema for you. It also fills in the corresponding object hook, so that marshmallow will create an instance of your Data Class on load (e.g. Person.schema().load returns a Person ) rather than a dict , which it does by default in marshmallow. Performance note .schema() is not cached (it generates the schema on every call), so if you have a nested Data Class you may want to save the result to a variable to avoid re-generation of the schema on every usage. person_schema = Person . schema () person_schema . dump ( people , many = True ) # later in the code... person_schema . dump ( person )","title":"Explanation"},{"location":"#override-the-default-encode-decode-marshmallow-field-of-a-specific-field","text":"See Overriding","title":"Override the default encode / decode / marshmallow field of a specific field?"},{"location":"#marshmallow-interop","text":"Using the dataclass_json decorator or mixing in DataClassJsonMixin will provide you with an additional method .schema() . .schema() generates a schema exactly equivalent to manually creating a marshmallow schema for your dataclass. You can reference the marshmallow API docs to learn other ways you can use the schema returned by .schema() . You can pass in the exact same arguments to .schema() that you would when constructing a PersonSchema instance, e.g. .schema(many=True) , and they will get passed through to the marshmallow schema. from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Person : name : str # You don't need to do this - it's generated for you by `.schema()`! from marshmallow import Schema , fields class PersonSchema ( Schema ): name = fields . Str ()","title":"Marshmallow interop"},{"location":"#overriding-extending","text":"","title":"Overriding / Extending"},{"location":"#overriding","text":"For example, you might want to encode/decode datetime objects using ISO format rather than the default timestamp . from dataclasses import dataclass , field from dataclasses_json import dataclass_json , config from datetime import datetime from marshmallow import fields @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : datetime = field ( metadata = config ( encoder = datetime . isoformat , decoder = datetime . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) ) )","title":"Overriding"},{"location":"#extending","text":"Similarly, you might want to extend dataclasses_json to encode date objects. from dataclasses import dataclass , field from dataclasses_json import dataclass_json , config from datetime import date from marshmallow import fields @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : date = field ( metadata = config ( encoder = date . isoformat , decoder = date . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) )) As you can see, you can override or extend the default codecs by providing a \"hook\" via a callable: - encoder : a callable, which will be invoked to convert the field value when encoding to JSON - decoder : a callable, which will be invoked to convert the JSON value when decoding from JSON - mm_field : a marshmallow field, which will affect the behavior of any operations involving .schema() Note that these hooks will be invoked regardless if you're using .to_json / dump / dumps and .from_json / load / loads . So apply overrides / extensions judiciously, making sure to carefully consider whether the interaction of the encode/decode/mm_field is consistent with what you expect!","title":"Extending"},{"location":"#what-if-i-have-other-dataclass-field-extensions-that-rely-on-metadata","text":"All the dataclasses_json.config does is return a mapping, namespaced under the key 'dataclasses_json' . Say there's another module, other_dataclass_package that uses metadata. Here's how you solve your problem: metadata = { 'other_dataclass_package' : 'some metadata...' } # pre-existing metadata for another dataclass package dataclass_json_config = config ( encoder = datetime . isoformat , decoder = datetime . fromisoformat , mm_field = fields . DateTime ( format = 'iso' ) ) metadata . update ( dataclass_json_config ) @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : datetime = field ( metadata = metadata ) You can also manually specify the dataclass_json configuration mapping. @dataclass_json @dataclass class DataClassWithIsoDatetime : created_at : date = field ( metadata = { 'dataclasses_json' : { 'encoder' : date . isoformat , 'decoder' : date . fromisoformat , 'mm_field' : fields . DateTime ( format = 'iso' ) }} )","title":"What if I have other dataclass field extensions that rely on metadata"},{"location":"#a-larger-example","text":"from dataclasses import dataclass from dataclasses_json import dataclass_json from typing import List @dataclass_json @dataclass ( frozen = True ) class Minion : name : str @dataclass_json @dataclass ( frozen = True ) class Boss : minions : List [ Minion ] boss = Boss ([ Minion ( 'evil minion' ), Minion ( 'very evil minion' )]) boss_json = \"\"\" { \"minions\": [ { \"name\": \"evil minion\" }, { \"name\": \"very evil minion\" } ] } \"\"\" . strip () assert boss . to_json ( indent = 4 ) == boss_json assert Boss . from_json ( boss_json ) == boss","title":"A larger example"},{"location":"#self-recursion","text":"Object hierarchies where fields are of the type that they are declared within require a small type hinting trick to declare the forward reference. from typing import Optional from dataclasses import dataclass from dataclasses_json import dataclass_json @dataclass_json @dataclass class Tree (): value : str left : Optional [ 'Tree' ] right : Optional [ 'Tree' ] Avoid using from __future__ import annotations as it will cause problems with the way dataclasses_json accesses the type annotations.","title":"Self Recursion"},{"location":"reference/dataclasses_json/","text":"Module dataclasses_json View Source from dataclasses_json.api import ( DataClassJsonMixin , LetterCase , config , dataclass_json ) Sub-modules dataclasses_json.api dataclasses_json.core dataclasses_json.mm dataclasses_json.utils","title":"Index"},{"location":"reference/dataclasses_json/#module-dataclasses_json","text":"View Source from dataclasses_json.api import ( DataClassJsonMixin , LetterCase , config , dataclass_json )","title":"Module dataclasses_json"},{"location":"reference/dataclasses_json/#sub-modules","text":"dataclasses_json.api dataclasses_json.core dataclasses_json.mm dataclasses_json.utils","title":"Sub-modules"},{"location":"reference/dataclasses_json/api/","text":"Module dataclasses_json.api View Source import abc import functools import json from enum import Enum from typing import ( Any , Callable , Dict , List , Optional , Tuple , Type , TypeVar , Union ) from marshmallow.fields import Field as MarshmallowField from stringcase import camelcase , snakecase , spinalcase , pascalcase from dataclasses_json.core import ( Json , _ExtendedEncoder , _asdict , _decode_dataclass ) from dataclasses_json.mm import JsonData , SchemaType , build_schema A = TypeVar ( 'A' ) B = TypeVar ( 'B' ) C = TypeVar ( 'C' ) Fields = List [ Tuple [ str , Any ]] class LetterCase ( Enum ): CAMEL = camelcase KEBAB = spinalcase SNAKE = snakecase PASCAL = pascalcase def config ( metadata : dict = None , * , encoder : callable = None , decoder : callable = None , mm_field : MarshmallowField = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ]: if metadata is None : metadata = {} data = metadata . setdefault ( 'dataclasses_json' , {}) if encoder is not None : data [ 'encoder' ] = encoder if decoder is not None : data [ 'decoder' ] = decoder if mm_field is not None : data [ 'mm_field' ] = mm_field if field_name is not None : if letter_case is not None : @functools.wraps ( letter_case ) def override ( _ , _letter_case = letter_case , _field_name = field_name ): return _letter_case ( _field_name ) else : def override ( _ , _field_name = field_name ): return _field_name letter_case = override if letter_case is not None : data [ 'letter_case' ] = letter_case return metadata class DataClassJsonMixin ( abc . ABC ): \"\"\" DataClassJsonMixin is an ABC that functions as a Mixin. As with other ABCs, it should not be instantiated directly. \"\"\" dataclass_json_config = None def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ), cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) @classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) @classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) def to_dict ( self , encode_json = False ): return _asdict ( self , encode_json = encode_json ) @classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = (), many : bool = False , context = None , load_only = (), dump_only = (), partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown ) def dataclass_json ( _cls = None , * , letter_case = None ): \"\"\" Based on the code in the `dataclasses` module to handle optional-parens decorators. See example below: @dataclass_json @dataclass_json(letter_case=Lettercase.CAMEL) class Example: ... \"\"\" def wrap ( cls ): return _process_class ( cls , letter_case ) if _cls is None : return wrap return wrap ( _cls ) def _process_class ( cls , letter_case ): if letter_case is not None : cls . dataclass_json_config = config ( letter_case = letter_case )[ 'dataclasses_json' ] cls . to_json = DataClassJsonMixin . to_json # unwrap and rewrap classmethod to tag it to cls rather than the literal # DataClassJsonMixin ABC cls . from_json = classmethod ( DataClassJsonMixin . from_json . __func__ ) cls . to_dict = DataClassJsonMixin . to_dict cls . from_dict = classmethod ( DataClassJsonMixin . from_dict . __func__ ) cls . schema = classmethod ( DataClassJsonMixin . schema . __func__ ) # register cls as a virtual subclass of DataClassJsonMixin DataClassJsonMixin . register ( cls ) return cls Functions config def ( metadata : dict = None , * , encoder : < built - in function callable > = None , decoder : < built - in function callable > = None , mm_field : marshmallow . fields . Field = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ] View Source def config ( metadata : dict = None , * , encoder : callable = None , decoder : callable = None , mm_field : MarshmallowField = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ]: if metadata is None : metadata = {} data = metadata . setdefault ( ' dataclasses_json ' , {} ) if encoder is not None : data [ ' encoder ' ] = encoder if decoder is not None : data [ ' decoder ' ] = decoder if mm_field is not None : data [ ' mm_field ' ] = mm_field if field_name is not None : if letter_case is not None : @ functools . wraps ( letter_case ) def override ( _ , _letter_case = letter_case , _field_name = field_name ) : return _letter_case ( _field_name ) else : def override ( _ , _field_name = field_name ) : return _field_name letter_case = override if letter_case is not None : data [ ' letter_case ' ] = letter_case return metadata dataclass_json def ( * , letter_case = None ) Based on the code in the dataclasses module to handle optional-parens decorators. See example below: @dataclass_json @dataclass_json(letter_case=Lettercase.CAMEL) class Example: ... View Source def dataclass_json ( _cls = None , * , letter_case = None ) : \"\"\" Based on the code in the ` dataclasses ` module to handle optional - parens decorators . See example below : @ dataclass_json @ dataclass_json ( letter_case = Lettercase . CAMEL ) class Example : ... \"\"\" def wrap ( cls ) : return _process_class ( cls , letter_case ) if _cls is None : return wrap return wrap ( _cls ) Classes DataClassJsonMixin class ( * args , ** kwargs ) DataClassJsonMixin is an ABC that functions as a Mixin. As with other ABCs, it should not be instantiated directly. Ancestors (in MRO) abc.ABC Class variables ``` python3 dataclass_json_config ``` : Static methods ##### from_dict ``` python3 def ( kvs : Union [ dict , list , str , int , float , bool , NoneType ], * , infer_missing = False ) -> ~ A ``` ??? example \" View Source \" @ classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) ##### from_json ``` python3 def ( s : Union [ str , bytes , bytearray ], * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> ~ A ``` ??? example \" View Source \" @ classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) ##### schema ``` python3 def ( * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> dataclasses_json . mm . SchemaF [ ~ A ] ``` ??? example \" View Source \" @ classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown ) Methods ##### to_dict ``` python3 def ( self , encode_json = False ) ``` ??? example \" View Source \" def to_dict ( self , encode_json = False ) : return _asdict ( self , encode_json = encode_json ) ##### to_json ``` python3 def ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Union [ int , str , NoneType ] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str ``` ??? example \" View Source \" def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ) , cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) View Source class DataClassJsonMixin ( abc . ABC ) : \"\"\" DataClassJsonMixin is an ABC that functions as a Mixin . As with other ABCs , it should not be instantiated directly . \"\"\" dataclass_json_config = None def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ) , cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) @ classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) @ classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) def to_dict ( self , encode_json = False ) : return _asdict ( self , encode_json = encode_json ) @ classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown ) LetterCase class ( * args , ** kwargs ) An enumeration. Ancestors (in MRO) enum.Enum View Source class LetterCase ( Enum ): CAMEL = camelcase KEBAB = spinalcase SNAKE = snakecase PASCAL = pascalcase","title":"API"},{"location":"reference/dataclasses_json/api/#module-dataclasses_jsonapi","text":"View Source import abc import functools import json from enum import Enum from typing import ( Any , Callable , Dict , List , Optional , Tuple , Type , TypeVar , Union ) from marshmallow.fields import Field as MarshmallowField from stringcase import camelcase , snakecase , spinalcase , pascalcase from dataclasses_json.core import ( Json , _ExtendedEncoder , _asdict , _decode_dataclass ) from dataclasses_json.mm import JsonData , SchemaType , build_schema A = TypeVar ( 'A' ) B = TypeVar ( 'B' ) C = TypeVar ( 'C' ) Fields = List [ Tuple [ str , Any ]] class LetterCase ( Enum ): CAMEL = camelcase KEBAB = spinalcase SNAKE = snakecase PASCAL = pascalcase def config ( metadata : dict = None , * , encoder : callable = None , decoder : callable = None , mm_field : MarshmallowField = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ]: if metadata is None : metadata = {} data = metadata . setdefault ( 'dataclasses_json' , {}) if encoder is not None : data [ 'encoder' ] = encoder if decoder is not None : data [ 'decoder' ] = decoder if mm_field is not None : data [ 'mm_field' ] = mm_field if field_name is not None : if letter_case is not None : @functools.wraps ( letter_case ) def override ( _ , _letter_case = letter_case , _field_name = field_name ): return _letter_case ( _field_name ) else : def override ( _ , _field_name = field_name ): return _field_name letter_case = override if letter_case is not None : data [ 'letter_case' ] = letter_case return metadata class DataClassJsonMixin ( abc . ABC ): \"\"\" DataClassJsonMixin is an ABC that functions as a Mixin. As with other ABCs, it should not be instantiated directly. \"\"\" dataclass_json_config = None def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ), cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) @classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) @classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) def to_dict ( self , encode_json = False ): return _asdict ( self , encode_json = encode_json ) @classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = (), many : bool = False , context = None , load_only = (), dump_only = (), partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown ) def dataclass_json ( _cls = None , * , letter_case = None ): \"\"\" Based on the code in the `dataclasses` module to handle optional-parens decorators. See example below: @dataclass_json @dataclass_json(letter_case=Lettercase.CAMEL) class Example: ... \"\"\" def wrap ( cls ): return _process_class ( cls , letter_case ) if _cls is None : return wrap return wrap ( _cls ) def _process_class ( cls , letter_case ): if letter_case is not None : cls . dataclass_json_config = config ( letter_case = letter_case )[ 'dataclasses_json' ] cls . to_json = DataClassJsonMixin . to_json # unwrap and rewrap classmethod to tag it to cls rather than the literal # DataClassJsonMixin ABC cls . from_json = classmethod ( DataClassJsonMixin . from_json . __func__ ) cls . to_dict = DataClassJsonMixin . to_dict cls . from_dict = classmethod ( DataClassJsonMixin . from_dict . __func__ ) cls . schema = classmethod ( DataClassJsonMixin . schema . __func__ ) # register cls as a virtual subclass of DataClassJsonMixin DataClassJsonMixin . register ( cls ) return cls","title":"Module dataclasses_json.api"},{"location":"reference/dataclasses_json/api/#functions","text":"","title":"Functions"},{"location":"reference/dataclasses_json/api/#config","text":"def ( metadata : dict = None , * , encoder : < built - in function callable > = None , decoder : < built - in function callable > = None , mm_field : marshmallow . fields . Field = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ] View Source def config ( metadata : dict = None , * , encoder : callable = None , decoder : callable = None , mm_field : MarshmallowField = None , letter_case : Callable [[ str ], str ] = None , field_name : str = None ) -> Dict [ str , dict ]: if metadata is None : metadata = {} data = metadata . setdefault ( ' dataclasses_json ' , {} ) if encoder is not None : data [ ' encoder ' ] = encoder if decoder is not None : data [ ' decoder ' ] = decoder if mm_field is not None : data [ ' mm_field ' ] = mm_field if field_name is not None : if letter_case is not None : @ functools . wraps ( letter_case ) def override ( _ , _letter_case = letter_case , _field_name = field_name ) : return _letter_case ( _field_name ) else : def override ( _ , _field_name = field_name ) : return _field_name letter_case = override if letter_case is not None : data [ ' letter_case ' ] = letter_case return metadata","title":"config"},{"location":"reference/dataclasses_json/api/#dataclass_json","text":"def ( * , letter_case = None ) Based on the code in the dataclasses module to handle optional-parens decorators. See example below: @dataclass_json @dataclass_json(letter_case=Lettercase.CAMEL) class Example: ... View Source def dataclass_json ( _cls = None , * , letter_case = None ) : \"\"\" Based on the code in the ` dataclasses ` module to handle optional - parens decorators . See example below : @ dataclass_json @ dataclass_json ( letter_case = Lettercase . CAMEL ) class Example : ... \"\"\" def wrap ( cls ) : return _process_class ( cls , letter_case ) if _cls is None : return wrap return wrap ( _cls )","title":"dataclass_json"},{"location":"reference/dataclasses_json/api/#classes","text":"","title":"Classes"},{"location":"reference/dataclasses_json/api/#dataclassjsonmixin","text":"class ( * args , ** kwargs ) DataClassJsonMixin is an ABC that functions as a Mixin. As with other ABCs, it should not be instantiated directly.","title":"DataClassJsonMixin"},{"location":"reference/dataclasses_json/api/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/dataclasses_json/api/#class-variables","text":"``` python3 dataclass_json_config ``` :","title":"Class variables"},{"location":"reference/dataclasses_json/api/#static-methods","text":"##### from_dict ``` python3 def ( kvs : Union [ dict , list , str , int , float , bool , NoneType ], * , infer_missing = False ) -> ~ A ``` ??? example \" View Source \" @ classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) ##### from_json ``` python3 def ( s : Union [ str , bytes , bytearray ], * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> ~ A ``` ??? example \" View Source \" @ classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) ##### schema ``` python3 def ( * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> dataclasses_json . mm . SchemaF [ ~ A ] ``` ??? example \" View Source \" @ classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown )","title":"Static methods"},{"location":"reference/dataclasses_json/api/#methods","text":"##### to_dict ``` python3 def ( self , encode_json = False ) ``` ??? example \" View Source \" def to_dict ( self , encode_json = False ) : return _asdict ( self , encode_json = encode_json ) ##### to_json ``` python3 def ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Union [ int , str , NoneType ] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str ``` ??? example \" View Source \" def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ) , cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) View Source class DataClassJsonMixin ( abc . ABC ) : \"\"\" DataClassJsonMixin is an ABC that functions as a Mixin . As with other ABCs , it should not be instantiated directly . \"\"\" dataclass_json_config = None def to_json ( self , * , skipkeys : bool = False , ensure_ascii : bool = True , check_circular : bool = True , allow_nan : bool = True , indent : Optional [ Union [ int , str ]] = None , separators : Tuple [ str , str ] = None , default : Callable = None , sort_keys : bool = False , ** kw ) -> str : return json . dumps ( self . to_dict ( encode_json = False ) , cls = _ExtendedEncoder , skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) @ classmethod def from_json ( cls : Type [ A ], s : JsonData , * , encoding = None , parse_float = None , parse_int = None , parse_constant = None , infer_missing = False , ** kw ) -> A : kvs = json . loads ( s , encoding = encoding , parse_float = parse_float , parse_int = parse_int , parse_constant = parse_constant , ** kw ) return cls . from_dict ( kvs , infer_missing = infer_missing ) @ classmethod def from_dict ( cls : Type [ A ], kvs : Json , * , infer_missing = False ) -> A : return _decode_dataclass ( cls , kvs , infer_missing ) def to_dict ( self , encode_json = False ) : return _asdict ( self , encode_json = encode_json ) @ classmethod def schema ( cls : Type [ A ], * , infer_missing : bool = False , only = None , exclude = () , many : bool = False , context = None , load_only = () , dump_only = () , partial : bool = False , unknown = None ) -> SchemaType : Schema = build_schema ( cls , DataClassJsonMixin , infer_missing , partial ) return Schema ( only = only , exclude = exclude , many = many , context = context , load_only = load_only , dump_only = dump_only , partial = partial , unknown = unknown )","title":"Methods"},{"location":"reference/dataclasses_json/api/#lettercase","text":"class ( * args , ** kwargs ) An enumeration.","title":"LetterCase"},{"location":"reference/dataclasses_json/api/#ancestors-in-mro_1","text":"enum.Enum View Source class LetterCase ( Enum ): CAMEL = camelcase KEBAB = spinalcase SNAKE = snakecase PASCAL = pascalcase","title":"Ancestors (in MRO)"},{"location":"reference/dataclasses_json/core/","text":"Module dataclasses_json.core View Source import copy import json import warnings from collections import namedtuple from dataclasses import ( MISSING , _is_dataclass_instance , fields , is_dataclass ) from datetime import datetime , timezone from decimal import Decimal from enum import Enum from typing import Collection , Mapping , Union , get_type_hints from uuid import UUID from typing_inspect import is_union_type from dataclasses_json.utils import ( _get_type_cons , _is_collection , _is_mapping , _is_new_type , _is_optional , _isinstance_safe , _issubclass_safe ) Json = Union [ dict , list , str , int , float , bool , None ] class _ExtendedEncoder ( json . JSONEncoder ): def default ( self , o ) -> Json : result : Json if _isinstance_safe ( o , Collection ): if _isinstance_safe ( o , Mapping ): result = dict ( o ) else : result = list ( o ) elif _isinstance_safe ( o , datetime ): result = o . timestamp () elif _isinstance_safe ( o , UUID ): result = str ( o ) elif _isinstance_safe ( o , Enum ): result = o . value elif _isinstance_safe ( o , Decimal ): result = str ( o ) else : result = json . JSONEncoder . default ( self , o ) return result def _user_overrides ( cls ): confs = [ 'encoder' , 'decoder' , 'mm_field' , 'letter_case' ] FieldOverride = namedtuple ( 'FieldOverride' , confs ) overrides = {} # overrides at the class-level try : cls_config = ( cls . dataclass_json_config if cls . dataclass_json_config is not None else {}) except AttributeError : cls_config = {} for field in fields ( cls ): field_config = field . metadata . get ( 'dataclasses_json' , {}) field_config . update ( cls_config ) overrides [ field . name ] = FieldOverride ( * map ( field_config . get , confs )) return overrides def _encode_json_type ( value , default = _ExtendedEncoder () . default ): if isinstance ( value , Json . __args__ ): return value return default ( value ) def _encode_overrides ( kvs , overrides , encode_json = False ): override_kvs = {} for k , v in kvs . items (): if k in overrides : letter_case = overrides [ k ] . letter_case original_key = k k = letter_case ( k ) if letter_case is not None else k encoder = overrides [ original_key ] . encoder v = encoder ( v ) if encoder is not None else v if encode_json : v = _encode_json_type ( v ) override_kvs [ k ] = v return override_kvs def _decode_letter_case_overrides ( field_names , overrides ): \"\"\"Override letter case of field names for encode/decode\"\"\" names = {} for field_name in field_names : field_override = overrides . get ( field_name ) if field_override is not None : letter_case = field_override . letter_case if letter_case is not None : names [ letter_case ( field_name )] = field_name return names def _decode_dataclass ( cls , kvs , infer_missing ): if isinstance ( kvs , cls ): return kvs overrides = _user_overrides ( cls ) kvs = {} if kvs is None and infer_missing else kvs field_names = [ field . name for field in fields ( cls )] decode_names = _decode_letter_case_overrides ( field_names , overrides ) kvs = { decode_names . get ( k , k ): v for k , v in kvs . items ()} missing_fields = { field for field in fields ( cls ) if field . name not in kvs } for field in missing_fields : if field . default is not MISSING : kvs [ field . name ] = field . default elif field . default_factory is not MISSING : kvs [ field . name ] = field . default_factory () elif infer_missing : kvs [ field . name ] = None init_kwargs = {} types = get_type_hints ( cls ) for field in fields ( cls ): # The field should be skipped from being added # to init_kwargs as it's not intended as a constructor argument. if not field . init : continue field_value = kvs [ field . name ] field_type = types [ field . name ] if field_value is None and not _is_optional ( field_type ): warning = ( f \"value of non-optional type {field.name} detected \" f \"when decoding {cls.__name__}\" ) if infer_missing : warnings . warn ( f \"Missing {warning} and was defaulted to None by \" f \"infer_missing=True. \" f \"Set infer_missing=False (the default) to prevent this \" f \"behavior.\" , RuntimeWarning ) else : warnings . warn ( f \"`NoneType` object {warning}.\" , RuntimeWarning ) init_kwargs [ field . name ] = field_value continue while True : if not _is_new_type ( field_type ): break field_type = field_type . __supertype__ if ( field . name in overrides and overrides [ field . name ] . decoder is not None ): # FIXME hack if field_type is type ( field_value ): init_kwargs [ field . name ] = field_value else : init_kwargs [ field . name ] = overrides [ field . name ] . decoder ( field_value ) elif is_dataclass ( field_type ): # FIXME this is a band-aid to deal with the value already being # serialized when handling nested marshmallow schema # proper fix is to investigate the marshmallow schema generation # code if is_dataclass ( field_value ): value = field_value else : value = _decode_dataclass ( field_type , field_value , infer_missing ) init_kwargs [ field . name ] = value elif _is_supported_generic ( field_type ) and field_type != str : init_kwargs [ field . name ] = _decode_generic ( field_type , field_value , infer_missing ) elif _issubclass_safe ( field_type , datetime ): # FIXME this is a hack to deal with mm already decoding # the issue is we want to leverage mm fields' missing argument # but need this for the object creation hook if isinstance ( field_value , datetime ): dt = field_value else : tz = datetime . now ( timezone . utc ) . astimezone () . tzinfo dt = datetime . fromtimestamp ( field_value , tz = tz ) init_kwargs [ field . name ] = dt elif _issubclass_safe ( field_type , Decimal ): init_kwargs [ field . name ] = ( field_value if isinstance ( field_value , Decimal ) else Decimal ( field_value )) elif _issubclass_safe ( field_type , UUID ): init_kwargs [ field . name ] = ( field_value if isinstance ( field_value , UUID ) else UUID ( field_value )) else : init_kwargs [ field . name ] = field_value return cls ( ** init_kwargs ) def _is_supported_generic ( type_ ): not_str = not _issubclass_safe ( type_ , str ) is_enum = _issubclass_safe ( type_ , Enum ) return ( not_str and _is_collection ( type_ )) or _is_optional ( type_ ) or is_union_type ( type_ ) or is_enum def _decode_generic ( type_ , value , infer_missing ): if value is None : res = value elif _issubclass_safe ( type_ , Enum ): # Convert to an Enum using the type as a constructor. Assumes a direct match is found. res = type_ ( value ) elif _is_collection ( type_ ): if _is_mapping ( type_ ): k_type , v_type = type_ . __args__ # a mapping type has `.keys()` and `.values()` (see collections.abc) ks = _decode_dict_keys ( k_type , value . keys (), infer_missing ) vs = _decode_items ( v_type , value . values (), infer_missing ) xs = zip ( ks , vs ) else : xs = _decode_items ( type_ . __args__ [ 0 ], value , infer_missing ) # get the constructor if using corresponding generic type in `typing` # otherwise fallback on constructing using type_ itself try : res = _get_type_cons ( type_ )( xs ) except TypeError : res = type_ ( xs ) else : # Optional or Union if _is_optional ( type_ ) and len ( type_ . __args__ ) == 2 : # Optional type_arg = type_ . __args__ [ 0 ] if is_dataclass ( type_arg ) or is_dataclass ( value ): res = _decode_dataclass ( type_arg , value , infer_missing ) elif _is_supported_generic ( type_arg ): res = _decode_generic ( type_arg , value , infer_missing ) else : res = value else : # Union (already decoded or unsupported 'from_json' used) res = value return res def _decode_dict_keys ( key_type , xs , infer_missing ): \"\"\" Because JSON object keys must be strs, we need the extra step of decoding them back into the user's chosen python type \"\"\" # handle NoneType keys... it's weird to type a Dict as NoneType keys # but it's valid... key_type = ( lambda x : x ) if key_type is type ( None ) else key_type return map ( key_type , _decode_items ( key_type , xs , infer_missing )) def _decode_items ( type_arg , xs , infer_missing ): \"\"\" This is a tricky situation where we need to check both the annotated type info (which is usually a type from `typing`) and check the value's type directly using `type()`. If the type_arg is a generic we can use the annotated type, but if the type_arg is a typevar we need to extract the reified type information hence the check of `is_dataclass(vs)` \"\"\" if is_dataclass ( type_arg ) or is_dataclass ( xs ): items = ( _decode_dataclass ( type_arg , x , infer_missing ) for x in xs ) elif _is_supported_generic ( type_arg ): items = ( _decode_generic ( type_arg , x , infer_missing ) for x in xs ) else : items = xs return items def _asdict ( obj , encode_json = False ): \"\"\" A re-implementation of `asdict` (based on the original in the `dataclasses` source) to support arbitrary Collection and Mapping types. \"\"\" if _is_dataclass_instance ( obj ): result = [] for field in fields ( obj ): value = _asdict ( getattr ( obj , field . name ), encode_json = encode_json ) result . append (( field . name , value )) return _encode_overrides ( dict ( result ), _user_overrides ( obj ), encode_json = encode_json ) elif isinstance ( obj , Mapping ): return dict (( _asdict ( k , encode_json = encode_json ), _asdict ( v , encode_json = encode_json )) for k , v in obj . items ()) elif isinstance ( obj , Collection ) and not isinstance ( obj , str ): return list ( _asdict ( v , encode_json = encode_json ) for v in obj ) else : return copy . deepcopy ( obj )","title":"Core"},{"location":"reference/dataclasses_json/core/#module-dataclasses_jsoncore","text":"View Source import copy import json import warnings from collections import namedtuple from dataclasses import ( MISSING , _is_dataclass_instance , fields , is_dataclass ) from datetime import datetime , timezone from decimal import Decimal from enum import Enum from typing import Collection , Mapping , Union , get_type_hints from uuid import UUID from typing_inspect import is_union_type from dataclasses_json.utils import ( _get_type_cons , _is_collection , _is_mapping , _is_new_type , _is_optional , _isinstance_safe , _issubclass_safe ) Json = Union [ dict , list , str , int , float , bool , None ] class _ExtendedEncoder ( json . JSONEncoder ): def default ( self , o ) -> Json : result : Json if _isinstance_safe ( o , Collection ): if _isinstance_safe ( o , Mapping ): result = dict ( o ) else : result = list ( o ) elif _isinstance_safe ( o , datetime ): result = o . timestamp () elif _isinstance_safe ( o , UUID ): result = str ( o ) elif _isinstance_safe ( o , Enum ): result = o . value elif _isinstance_safe ( o , Decimal ): result = str ( o ) else : result = json . JSONEncoder . default ( self , o ) return result def _user_overrides ( cls ): confs = [ 'encoder' , 'decoder' , 'mm_field' , 'letter_case' ] FieldOverride = namedtuple ( 'FieldOverride' , confs ) overrides = {} # overrides at the class-level try : cls_config = ( cls . dataclass_json_config if cls . dataclass_json_config is not None else {}) except AttributeError : cls_config = {} for field in fields ( cls ): field_config = field . metadata . get ( 'dataclasses_json' , {}) field_config . update ( cls_config ) overrides [ field . name ] = FieldOverride ( * map ( field_config . get , confs )) return overrides def _encode_json_type ( value , default = _ExtendedEncoder () . default ): if isinstance ( value , Json . __args__ ): return value return default ( value ) def _encode_overrides ( kvs , overrides , encode_json = False ): override_kvs = {} for k , v in kvs . items (): if k in overrides : letter_case = overrides [ k ] . letter_case original_key = k k = letter_case ( k ) if letter_case is not None else k encoder = overrides [ original_key ] . encoder v = encoder ( v ) if encoder is not None else v if encode_json : v = _encode_json_type ( v ) override_kvs [ k ] = v return override_kvs def _decode_letter_case_overrides ( field_names , overrides ): \"\"\"Override letter case of field names for encode/decode\"\"\" names = {} for field_name in field_names : field_override = overrides . get ( field_name ) if field_override is not None : letter_case = field_override . letter_case if letter_case is not None : names [ letter_case ( field_name )] = field_name return names def _decode_dataclass ( cls , kvs , infer_missing ): if isinstance ( kvs , cls ): return kvs overrides = _user_overrides ( cls ) kvs = {} if kvs is None and infer_missing else kvs field_names = [ field . name for field in fields ( cls )] decode_names = _decode_letter_case_overrides ( field_names , overrides ) kvs = { decode_names . get ( k , k ): v for k , v in kvs . items ()} missing_fields = { field for field in fields ( cls ) if field . name not in kvs } for field in missing_fields : if field . default is not MISSING : kvs [ field . name ] = field . default elif field . default_factory is not MISSING : kvs [ field . name ] = field . default_factory () elif infer_missing : kvs [ field . name ] = None init_kwargs = {} types = get_type_hints ( cls ) for field in fields ( cls ): # The field should be skipped from being added # to init_kwargs as it's not intended as a constructor argument. if not field . init : continue field_value = kvs [ field . name ] field_type = types [ field . name ] if field_value is None and not _is_optional ( field_type ): warning = ( f \"value of non-optional type {field.name} detected \" f \"when decoding {cls.__name__}\" ) if infer_missing : warnings . warn ( f \"Missing {warning} and was defaulted to None by \" f \"infer_missing=True. \" f \"Set infer_missing=False (the default) to prevent this \" f \"behavior.\" , RuntimeWarning ) else : warnings . warn ( f \"`NoneType` object {warning}.\" , RuntimeWarning ) init_kwargs [ field . name ] = field_value continue while True : if not _is_new_type ( field_type ): break field_type = field_type . __supertype__ if ( field . name in overrides and overrides [ field . name ] . decoder is not None ): # FIXME hack if field_type is type ( field_value ): init_kwargs [ field . name ] = field_value else : init_kwargs [ field . name ] = overrides [ field . name ] . decoder ( field_value ) elif is_dataclass ( field_type ): # FIXME this is a band-aid to deal with the value already being # serialized when handling nested marshmallow schema # proper fix is to investigate the marshmallow schema generation # code if is_dataclass ( field_value ): value = field_value else : value = _decode_dataclass ( field_type , field_value , infer_missing ) init_kwargs [ field . name ] = value elif _is_supported_generic ( field_type ) and field_type != str : init_kwargs [ field . name ] = _decode_generic ( field_type , field_value , infer_missing ) elif _issubclass_safe ( field_type , datetime ): # FIXME this is a hack to deal with mm already decoding # the issue is we want to leverage mm fields' missing argument # but need this for the object creation hook if isinstance ( field_value , datetime ): dt = field_value else : tz = datetime . now ( timezone . utc ) . astimezone () . tzinfo dt = datetime . fromtimestamp ( field_value , tz = tz ) init_kwargs [ field . name ] = dt elif _issubclass_safe ( field_type , Decimal ): init_kwargs [ field . name ] = ( field_value if isinstance ( field_value , Decimal ) else Decimal ( field_value )) elif _issubclass_safe ( field_type , UUID ): init_kwargs [ field . name ] = ( field_value if isinstance ( field_value , UUID ) else UUID ( field_value )) else : init_kwargs [ field . name ] = field_value return cls ( ** init_kwargs ) def _is_supported_generic ( type_ ): not_str = not _issubclass_safe ( type_ , str ) is_enum = _issubclass_safe ( type_ , Enum ) return ( not_str and _is_collection ( type_ )) or _is_optional ( type_ ) or is_union_type ( type_ ) or is_enum def _decode_generic ( type_ , value , infer_missing ): if value is None : res = value elif _issubclass_safe ( type_ , Enum ): # Convert to an Enum using the type as a constructor. Assumes a direct match is found. res = type_ ( value ) elif _is_collection ( type_ ): if _is_mapping ( type_ ): k_type , v_type = type_ . __args__ # a mapping type has `.keys()` and `.values()` (see collections.abc) ks = _decode_dict_keys ( k_type , value . keys (), infer_missing ) vs = _decode_items ( v_type , value . values (), infer_missing ) xs = zip ( ks , vs ) else : xs = _decode_items ( type_ . __args__ [ 0 ], value , infer_missing ) # get the constructor if using corresponding generic type in `typing` # otherwise fallback on constructing using type_ itself try : res = _get_type_cons ( type_ )( xs ) except TypeError : res = type_ ( xs ) else : # Optional or Union if _is_optional ( type_ ) and len ( type_ . __args__ ) == 2 : # Optional type_arg = type_ . __args__ [ 0 ] if is_dataclass ( type_arg ) or is_dataclass ( value ): res = _decode_dataclass ( type_arg , value , infer_missing ) elif _is_supported_generic ( type_arg ): res = _decode_generic ( type_arg , value , infer_missing ) else : res = value else : # Union (already decoded or unsupported 'from_json' used) res = value return res def _decode_dict_keys ( key_type , xs , infer_missing ): \"\"\" Because JSON object keys must be strs, we need the extra step of decoding them back into the user's chosen python type \"\"\" # handle NoneType keys... it's weird to type a Dict as NoneType keys # but it's valid... key_type = ( lambda x : x ) if key_type is type ( None ) else key_type return map ( key_type , _decode_items ( key_type , xs , infer_missing )) def _decode_items ( type_arg , xs , infer_missing ): \"\"\" This is a tricky situation where we need to check both the annotated type info (which is usually a type from `typing`) and check the value's type directly using `type()`. If the type_arg is a generic we can use the annotated type, but if the type_arg is a typevar we need to extract the reified type information hence the check of `is_dataclass(vs)` \"\"\" if is_dataclass ( type_arg ) or is_dataclass ( xs ): items = ( _decode_dataclass ( type_arg , x , infer_missing ) for x in xs ) elif _is_supported_generic ( type_arg ): items = ( _decode_generic ( type_arg , x , infer_missing ) for x in xs ) else : items = xs return items def _asdict ( obj , encode_json = False ): \"\"\" A re-implementation of `asdict` (based on the original in the `dataclasses` source) to support arbitrary Collection and Mapping types. \"\"\" if _is_dataclass_instance ( obj ): result = [] for field in fields ( obj ): value = _asdict ( getattr ( obj , field . name ), encode_json = encode_json ) result . append (( field . name , value )) return _encode_overrides ( dict ( result ), _user_overrides ( obj ), encode_json = encode_json ) elif isinstance ( obj , Mapping ): return dict (( _asdict ( k , encode_json = encode_json ), _asdict ( v , encode_json = encode_json )) for k , v in obj . items ()) elif isinstance ( obj , Collection ) and not isinstance ( obj , str ): return list ( _asdict ( v , encode_json = encode_json ) for v in obj ) else : return copy . deepcopy ( obj )","title":"Module dataclasses_json.core"},{"location":"reference/dataclasses_json/mm/","text":"Module dataclasses_json.mm View Source import typing import warnings import sys from copy import deepcopy from dataclasses import MISSING , is_dataclass , fields as dc_fields from datetime import datetime from decimal import Decimal from uuid import UUID from enum import Enum from typing_inspect import is_union_type from marshmallow import fields , Schema , post_load from marshmallow_enum import EnumField from dataclasses_json.core import ( _is_supported_generic , _decode_dataclass , _ExtendedEncoder , _user_overrides ) from dataclasses_json.utils import ( _is_collection , _is_optional , _issubclass_safe , _timestamp_to_dt_aware , _is_new_type , _get_type_origin ) class _TimestampField ( fields . Field ): def _serialize ( self , value , attr , obj , ** kwargs ): return value . timestamp () def _deserialize ( self , value , attr , data , ** kwargs ): return _timestamp_to_dt_aware ( value ) class _IsoField ( fields . Field ): def _serialize ( self , value , attr , obj , ** kwargs ): return value . isoformat () def _deserialize ( self , value , attr , data , ** kwargs ): return datetime . fromisoformat ( value ) class _UnionField ( fields . Field ): def __init__ ( self , desc , cls , field , * args , ** kwargs ): self . desc = desc self . cls = cls self . field = field super () . __init__ ( * args , ** kwargs ) def _serialize ( self , value , attr , obj , ** kwargs ): if self . allow_none and value is None : return None for type_ , schema_ in self . desc . items (): if _issubclass_safe ( type ( value ), type_ ): if is_dataclass ( value ): res = schema_ . _serialize ( value , attr , obj , ** kwargs ) res [ '__type' ] = str ( type_ . __name__ ) return res break elif isinstance ( value , _get_type_origin ( type_ )): return schema_ . _serialize ( value , attr , obj , ** kwargs ) else : warnings . warn ( f 'The type \"{type(value).__name__}\" (value: \"{value}\") ' f 'is not in the list of possible types of typing.Union ' f '(dataclass: {self.cls.__name__}, field: {self.field.name}). ' f 'Value cannot be serialized properly.' ) return super () . _serialize ( value , attr , obj , ** kwargs ) def _deserialize ( self , value , attr , data , ** kwargs ): tmp_value = deepcopy ( value ) if isinstance ( tmp_value , dict ) and '__type' in tmp_value : dc_name = tmp_value [ '__type' ] for type_ , schema_ in self . desc . items (): if is_dataclass ( type_ ) and type_ . __name__ == dc_name : del tmp_value [ '__type' ] return schema_ . _deserialize ( tmp_value , attr , data , ** kwargs ) for type_ , schema_ in self . desc . items (): if isinstance ( tmp_value , _get_type_origin ( type_ )): return schema_ . _deserialize ( tmp_value , attr , data , ** kwargs ) else : warnings . warn ( f 'The type \"{type(tmp_value).__name__}\" (value: \"{tmp_value}\") ' f 'is not in the list of possible types of typing.Union ' f '(dataclass: {self.cls.__name__}, field: {self.field.name}). ' f 'Value cannot be deserialized properly.' ) return super () . _deserialize ( tmp_value , attr , data , ** kwargs ) TYPES = { typing . Mapping : fields . Mapping , typing . MutableMapping : fields . Mapping , typing . List : fields . List , typing . Dict : fields . Dict , typing . Tuple : fields . Tuple , typing . Callable : fields . Function , dict : fields . Dict , list : fields . List , str : fields . Str , int : fields . Int , float : fields . Float , bool : fields . Bool , datetime : _TimestampField , UUID : fields . UUID , Decimal : fields . Decimal } A = typing . TypeVar ( 'A' ) JsonData = typing . Union [ str , bytes , bytearray ] TEncoded = typing . Dict [ str , typing . Any ] TOneOrMulti = typing . Union [ typing . List [ A ], A ] TOneOrMultiEncoded = typing . Union [ typing . List [ TEncoded ], TEncoded ] if sys . version_info >= ( 3 , 7 ): class SchemaF ( Schema , typing . Generic [ A ]): \"\"\"Lift Schema into a type constructor\"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Raises exception because this class should not be inherited. This class is helper only. \"\"\" super () . __init__ ( * args , ** kwargs ) raise NotImplementedError () @typing.overload def dump ( self , obj : typing . List [ A ], many : bool = None ) -> typing . List [ TEncoded ]: pass @typing.overload def dump ( self , obj : A , many : bool = None ) -> TEncoded : pass def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass @typing.overload def dumps ( self , obj : typing . List [ A ], many : bool = None , * args , ** kwargs ) -> str : pass @typing.overload def dumps ( self , obj : A , many : bool = None , * args , ** kwargs ) -> str : pass def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass @typing.overload def load ( self , data : typing . List [ TEncoded ], many : bool = True , partial : bool = None , unknown : bool = None ) -> \\ typing . List [ A ]: pass @typing.overload def load ( self , data : TEncoded , many : None = None , partial : bool = None , unknown : bool = None ) -> A : pass def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass @typing.overload def loads ( self , json_data : JsonData , many : bool = True , partial : bool = None , unknown : bool = None , ** kwargs ) -> typing . List [ A ]: pass @typing.overload def loads ( self , json_data : JsonData , many : None = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> A : pass def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass SchemaType = SchemaF [ A ] else : SchemaType = Schema def build_type ( type_ , options , mixin , field , cls ): def inner ( type_ , options ): while True : if not _is_new_type ( type_ ): break type_ = type_ . __supertype__ if is_dataclass ( type_ ): if _issubclass_safe ( type_ , mixin ): options [ 'field_many' ] = bool ( _is_supported_generic ( field . type ) and _is_collection ( field . type )) return fields . Nested ( type_ . schema (), ** options ) else : warnings . warn ( f \"Nested dataclass field {field.name} of type \" f \"{field.type} detected in \" f \"{cls.__name__} that is not an instance of \" f \"dataclass_json. Did you mean to recursively \" f \"serialize this field? If so, make sure to \" f \"augment {type_} with either the \" f \"`dataclass_json` decorator or mixin.\" ) return fields . Field ( ** options ) origin = getattr ( type_ , '__origin__' , type_ ) args = [ inner ( a , {}) for a in getattr ( type_ , '__args__' , []) if a is not type ( None )] if origin in TYPES : return TYPES [ origin ]( * args , ** options ) if _issubclass_safe ( origin , Enum ): return EnumField ( enum = origin , by_value = True , * args , ** options ) if is_union_type ( type_ ): union_types = [ a for a in getattr ( type_ , '__args__' , []) if a is not type ( None )] union_desc = dict ( zip ( union_types , args )) return _UnionField ( union_desc , cls , field , ** options ) warnings . warn ( f \"Unknown type {type_} at {cls.__name__}.{field.name}: {field.type} \" f \"It's advised to pass the correct marshmallow type to `mm_field`.\" ) return fields . Field ( ** options ) return inner ( type_ , options ) def schema ( cls , mixin , infer_missing ): schema = {} overrides = _user_overrides ( cls ) for field in dc_fields ( cls ): metadata = ( field . metadata or {}) . get ( 'dataclasses_json' , {}) metadata = overrides [ field . name ] if metadata . mm_field is not None : schema [ field . name ] = metadata . mm_field else : type_ = field . type options = {} missing_key = 'missing' if infer_missing else 'default' if field . default is not MISSING : options [ missing_key ] = field . default elif field . default_factory is not MISSING : options [ missing_key ] = field . default_factory if options . get ( missing_key , ... ) is None : options [ 'allow_none' ] = True if _is_optional ( type_ ): options . setdefault ( missing_key , None ) options [ 'allow_none' ] = True if len ( type_ . __args__ ) == 2 : # Union[str, int, None] is optional too, but it has more than 1 typed field. type_ = type_ . __args__ [ 0 ] if metadata . letter_case is not None : options [ 'data_key' ] = metadata . letter_case ( field . name ) t = build_type ( type_ , options , mixin , field , cls ) # if type(t) is not fields.Field: # If we use `isinstance` we would return nothing. schema [ field . name ] = t return schema def build_schema ( cls : typing . Type [ A ], mixin , infer_missing , partial ) -> typing . Type [ SchemaType ]: Meta = type ( 'Meta' , (), { 'fields' : tuple ( field . name for field in dc_fields ( cls ) if field . name != 'dataclass_json_config' )}) @post_load def make_instance ( self , kvs , ** kwargs ): return _decode_dataclass ( cls , kvs , partial ) def dumps ( self , * args , ** kwargs ): if 'cls' not in kwargs : kwargs [ 'cls' ] = _ExtendedEncoder return Schema . dumps ( self , * args , ** kwargs ) schema_ = schema ( cls , mixin , infer_missing ) DataClassSchema : typing . Type [ SchemaType ] = type ( f '{cls.__name__.capitalize()}Schema' , ( Schema ,), { 'Meta' : Meta , f 'make_{cls.__name__.lower()}' : make_instance , 'dumps' : dumps , ** schema_ }) return DataClassSchema Functions build_schema def ( cls : Type [ ~ A ], mixin , infer_missing , partial ) -> Type [ dataclasses_json . mm . SchemaF [ ~ A ]] View Source def build_schema ( cls : typing . Type [ A ], mixin , infer_missing , partial ) -> typing . Type [ SchemaType ]: Meta = type ( ' Meta ' , () , { ' fields ' : tuple ( field . name for field in dc_fields ( cls ) if field . name != ' dataclass_json_config ' ) } ) @ post_load def make_instance ( self , kvs , ** kwargs ) : return _decode_dataclass ( cls , kvs , partial ) def dumps ( self , * args , ** kwargs ) : if ' cls ' not in kwargs : kwargs [ ' cls ' ] = _ExtendedEncoder return Schema . dumps ( self , * args , ** kwargs ) schema_ = schema ( cls , mixin , infer_missing ) DataClassSchema : typing . Type [ SchemaType ] = type ( f ' {cls.__name__.capitalize()}Schema ' , ( Schema , ) , { ' Meta ' : Meta , f ' make_{cls.__name__.lower()} ' : make_instance , ' dumps ' : dumps , ** schema_ } ) return DataClassSchema build_type def ( type_ , options , mixin , field , cls ) View Source def build_type ( type_ , options , mixin , field , cls ) : def inner ( type_ , options ) : while True : if not _is_new_type ( type_ ) : break type_ = type_ . __supertype__ if is_dataclass ( type_ ) : if _issubclass_safe ( type_ , mixin ) : options [ ' field_many ' ] = bool ( _is_supported_generic ( field . type ) and _is_collection ( field . type )) return fields . Nested ( type_ . schema () , ** options ) else : warnings . warn ( f \" Nested dataclass field {field.name} of type \" f \" {field.type} detected in \" f \" {cls.__name__} that is not an instance of \" f \" dataclass_json. Did you mean to recursively \" f \" serialize this field? If so, make sure to \" f \" augment {type_} with either the \" f \" `dataclass_json` decorator or mixin. \" ) return fields . Field ( ** options ) origin = getattr ( type_ , ' __origin__ ' , type_ ) args = [ inner ( a , {} ) for a in getattr ( type_ , ' __args__ ' , [] ) if a is not type ( None ) ] if origin in TYPES : return TYPES [ origin ] ( * args , ** options ) if _issubclass_safe ( origin , Enum ) : return EnumField ( enum = origin , by_value = True , * args , ** options ) if is_union_type ( type_ ) : union_types = [ a for a in getattr ( type_ , ' __args__ ' , [] ) if a is not type ( None ) ] union_desc = dict ( zip ( union_types , args )) return _UnionField ( union_desc , cls , field , ** options ) warnings . warn ( f \" Unknown type {type_} at {cls.__name__}.{field.name}: {field.type} \" f \" It's advised to pass the correct marshmallow type to `mm_field`. \" ) return fields . Field ( ** options ) return inner ( type_ , options ) schema def ( cls , mixin , infer_missing ) View Source def schema ( cls , mixin , infer_missing ) : schema = {} overrides = _user_overrides ( cls ) for field in dc_fields ( cls ) : metadata = ( field . metadata or {} ) . get ( ' dataclasses_json ' , {} ) metadata = overrides [ field . name ] if metadata . mm_field is not None : schema [ field . name ] = metadata . mm_field else : type_ = field . type options = {} missing_key = ' missing ' if infer_missing else ' default ' if field . default is not MISSING : options [ missing_key ] = field . default elif field . default_factory is not MISSING : options [ missing_key ] = field . default_factory if options . get ( missing_key , ... ) is None : options [ ' allow_none ' ] = True if _is_optional ( type_ ) : options . setdefault ( missing_key , None ) options [ ' allow_none ' ] = True if len ( type_ . __args__ ) == 2 : # Union [ str , int , None ] is optional too , but it has more than 1 typed field . type_ = type_ . __args__ [ 0 ] if metadata . letter_case is not None : options [ ' data_key ' ] = metadata . letter_case ( field . name ) t = build_type ( type_ , options , mixin , field , cls ) # if type ( t ) is not fields . Field : # If we use ` isinstance ` we would return nothing . schema [ field . name ] = t return schema Classes SchemaF class ( * args , ** kwargs ) Lift Schema into a type constructor Raises exception because this class should not be inherited. This class is helper only. Ancestors (in MRO) marshmallow.schema.Schema marshmallow.schema.BaseSchema marshmallow.base.SchemaABC typing.Generic Class variables ``` python3 opts ``` : Methods ##### dump ``` python3 def ( self , obj : Union [ List [ ~ A ], ~ A ], many : bool = None ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] ``` Serialize an object to native Python data types according to this Schema ' s fields. :param obj : The object to serialize . :param bool many : Whether to serialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :return : A dict of serialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the serialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if `` obj `` is invalid . .. versionchanged :: 3 . 0 . 0 rc9 Validation no longer occurs upon serialization . ??? example \" View Source \" def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass ##### dumps ``` python3 def ( self , obj : Union [ List [ ~ A ], ~ A ], many : bool = None , * args , ** kwargs ) -> str ``` Same as : meth :` dump `, except return a JSON - encoded string . :param obj : The object to serialize . :param bool many : Whether to serialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :return : A `` json `` string :rtype : str .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the serialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if `` obj `` is invalid . ??? example \" View Source \" def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass ##### load ``` python3 def ( self , data : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], many : bool = None , partial : bool = None , unknown : bool = None ) -> Union [ List [ ~ A ], ~ A ] ``` Deserialize a data structure to an object defined by this Schema ' s fields. :param dict data : The data to deserialize . :param bool many : Whether to deserialize ` data ` as a collection . If ` None `, the value for ` self . many ` is used . :param bool | tuple partial : Whether to ignore missing fields and not require any fields declared . Propagates down to `` Nested `` fields as well . If its value is an iterable , only missing fields listed in that iterable will be ignored . Use dot delimiters to specify nested fields . :param unknown : Whether to exclude , include , or raise an error for unknown fields in the data . Use ` EXCLUDE `, ` INCLUDE ` or ` RAISE `. If ` None `, the value for ` self . unknown ` is used . :return : A dict of deserialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the deserialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if invalid data are passed . ??? example \" View Source \" def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass ##### loads ``` python3 def ( self , json_data : Union [ str , bytes , bytearray ], many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> Union [ List [ ~ A ], ~ A ] ``` Same as : meth :` load `, except it takes a JSON string as input . :param str json_data : A JSON string of the data to deserialize . :param bool many : Whether to deserialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :param bool | tuple partial : Whether to ignore missing fields and not require any fields declared . Propagates down to `` Nested `` fields as well . If its value is an iterable , only missing fields listed in that iterable will be ignored . Use dot delimiters to specify nested fields . :param unknown : Whether to exclude , include , or raise an error for unknown fields in the data . Use ` EXCLUDE `, ` INCLUDE ` or ` RAISE `. If ` None `, the value for ` self . unknown ` is used . :return : A dict of deserialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the deserialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if invalid data are passed . ??? example \" View Source \" def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass View Source class SchemaF ( Schema , typing . Generic [ A ] ) : \"\"\"Lift Schema into a type constructor\"\"\" def __init__ ( self , * args , ** kwargs ) : \"\"\" Raises exception because this class should not be inherited. This class is helper only. \"\"\" super (). __init__ ( * args , ** kwargs ) raise NotImplementedError () @typing . overload def dump ( self , obj : typing . List [ A ] , many : bool = None ) -> typing . List [ TEncoded ] : pass @typing . overload def dump ( self , obj : A , many : bool = None ) -> TEncoded : pass def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass @typing . overload def dumps ( self , obj : typing . List [ A ] , many : bool = None , * args , ** kwargs ) -> str : pass @typing . overload def dumps ( self , obj : A , many : bool = None , * args , ** kwargs ) -> str : pass def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass @typing . overload def load ( self , data : typing . List [ TEncoded ] , many : bool = True , partial : bool = None , unknown : bool = None ) -> \\ typing . List [ A ] : pass @typing . overload def load ( self , data : TEncoded , many : None = None , partial : bool = None , unknown : bool = None ) -> A : pass def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass @typing . overload def loads ( self , json_data : JsonData , many : bool = True , partial : bool = None , unknown : bool = None , ** kwargs ) -> typing . List [ A ] : pass @typing . overload def loads ( self , json_data : JsonData , many : None = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> A : pass def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass","title":"Mm"},{"location":"reference/dataclasses_json/mm/#module-dataclasses_jsonmm","text":"View Source import typing import warnings import sys from copy import deepcopy from dataclasses import MISSING , is_dataclass , fields as dc_fields from datetime import datetime from decimal import Decimal from uuid import UUID from enum import Enum from typing_inspect import is_union_type from marshmallow import fields , Schema , post_load from marshmallow_enum import EnumField from dataclasses_json.core import ( _is_supported_generic , _decode_dataclass , _ExtendedEncoder , _user_overrides ) from dataclasses_json.utils import ( _is_collection , _is_optional , _issubclass_safe , _timestamp_to_dt_aware , _is_new_type , _get_type_origin ) class _TimestampField ( fields . Field ): def _serialize ( self , value , attr , obj , ** kwargs ): return value . timestamp () def _deserialize ( self , value , attr , data , ** kwargs ): return _timestamp_to_dt_aware ( value ) class _IsoField ( fields . Field ): def _serialize ( self , value , attr , obj , ** kwargs ): return value . isoformat () def _deserialize ( self , value , attr , data , ** kwargs ): return datetime . fromisoformat ( value ) class _UnionField ( fields . Field ): def __init__ ( self , desc , cls , field , * args , ** kwargs ): self . desc = desc self . cls = cls self . field = field super () . __init__ ( * args , ** kwargs ) def _serialize ( self , value , attr , obj , ** kwargs ): if self . allow_none and value is None : return None for type_ , schema_ in self . desc . items (): if _issubclass_safe ( type ( value ), type_ ): if is_dataclass ( value ): res = schema_ . _serialize ( value , attr , obj , ** kwargs ) res [ '__type' ] = str ( type_ . __name__ ) return res break elif isinstance ( value , _get_type_origin ( type_ )): return schema_ . _serialize ( value , attr , obj , ** kwargs ) else : warnings . warn ( f 'The type \"{type(value).__name__}\" (value: \"{value}\") ' f 'is not in the list of possible types of typing.Union ' f '(dataclass: {self.cls.__name__}, field: {self.field.name}). ' f 'Value cannot be serialized properly.' ) return super () . _serialize ( value , attr , obj , ** kwargs ) def _deserialize ( self , value , attr , data , ** kwargs ): tmp_value = deepcopy ( value ) if isinstance ( tmp_value , dict ) and '__type' in tmp_value : dc_name = tmp_value [ '__type' ] for type_ , schema_ in self . desc . items (): if is_dataclass ( type_ ) and type_ . __name__ == dc_name : del tmp_value [ '__type' ] return schema_ . _deserialize ( tmp_value , attr , data , ** kwargs ) for type_ , schema_ in self . desc . items (): if isinstance ( tmp_value , _get_type_origin ( type_ )): return schema_ . _deserialize ( tmp_value , attr , data , ** kwargs ) else : warnings . warn ( f 'The type \"{type(tmp_value).__name__}\" (value: \"{tmp_value}\") ' f 'is not in the list of possible types of typing.Union ' f '(dataclass: {self.cls.__name__}, field: {self.field.name}). ' f 'Value cannot be deserialized properly.' ) return super () . _deserialize ( tmp_value , attr , data , ** kwargs ) TYPES = { typing . Mapping : fields . Mapping , typing . MutableMapping : fields . Mapping , typing . List : fields . List , typing . Dict : fields . Dict , typing . Tuple : fields . Tuple , typing . Callable : fields . Function , dict : fields . Dict , list : fields . List , str : fields . Str , int : fields . Int , float : fields . Float , bool : fields . Bool , datetime : _TimestampField , UUID : fields . UUID , Decimal : fields . Decimal } A = typing . TypeVar ( 'A' ) JsonData = typing . Union [ str , bytes , bytearray ] TEncoded = typing . Dict [ str , typing . Any ] TOneOrMulti = typing . Union [ typing . List [ A ], A ] TOneOrMultiEncoded = typing . Union [ typing . List [ TEncoded ], TEncoded ] if sys . version_info >= ( 3 , 7 ): class SchemaF ( Schema , typing . Generic [ A ]): \"\"\"Lift Schema into a type constructor\"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Raises exception because this class should not be inherited. This class is helper only. \"\"\" super () . __init__ ( * args , ** kwargs ) raise NotImplementedError () @typing.overload def dump ( self , obj : typing . List [ A ], many : bool = None ) -> typing . List [ TEncoded ]: pass @typing.overload def dump ( self , obj : A , many : bool = None ) -> TEncoded : pass def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass @typing.overload def dumps ( self , obj : typing . List [ A ], many : bool = None , * args , ** kwargs ) -> str : pass @typing.overload def dumps ( self , obj : A , many : bool = None , * args , ** kwargs ) -> str : pass def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass @typing.overload def load ( self , data : typing . List [ TEncoded ], many : bool = True , partial : bool = None , unknown : bool = None ) -> \\ typing . List [ A ]: pass @typing.overload def load ( self , data : TEncoded , many : None = None , partial : bool = None , unknown : bool = None ) -> A : pass def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass @typing.overload def loads ( self , json_data : JsonData , many : bool = True , partial : bool = None , unknown : bool = None , ** kwargs ) -> typing . List [ A ]: pass @typing.overload def loads ( self , json_data : JsonData , many : None = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> A : pass def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass SchemaType = SchemaF [ A ] else : SchemaType = Schema def build_type ( type_ , options , mixin , field , cls ): def inner ( type_ , options ): while True : if not _is_new_type ( type_ ): break type_ = type_ . __supertype__ if is_dataclass ( type_ ): if _issubclass_safe ( type_ , mixin ): options [ 'field_many' ] = bool ( _is_supported_generic ( field . type ) and _is_collection ( field . type )) return fields . Nested ( type_ . schema (), ** options ) else : warnings . warn ( f \"Nested dataclass field {field.name} of type \" f \"{field.type} detected in \" f \"{cls.__name__} that is not an instance of \" f \"dataclass_json. Did you mean to recursively \" f \"serialize this field? If so, make sure to \" f \"augment {type_} with either the \" f \"`dataclass_json` decorator or mixin.\" ) return fields . Field ( ** options ) origin = getattr ( type_ , '__origin__' , type_ ) args = [ inner ( a , {}) for a in getattr ( type_ , '__args__' , []) if a is not type ( None )] if origin in TYPES : return TYPES [ origin ]( * args , ** options ) if _issubclass_safe ( origin , Enum ): return EnumField ( enum = origin , by_value = True , * args , ** options ) if is_union_type ( type_ ): union_types = [ a for a in getattr ( type_ , '__args__' , []) if a is not type ( None )] union_desc = dict ( zip ( union_types , args )) return _UnionField ( union_desc , cls , field , ** options ) warnings . warn ( f \"Unknown type {type_} at {cls.__name__}.{field.name}: {field.type} \" f \"It's advised to pass the correct marshmallow type to `mm_field`.\" ) return fields . Field ( ** options ) return inner ( type_ , options ) def schema ( cls , mixin , infer_missing ): schema = {} overrides = _user_overrides ( cls ) for field in dc_fields ( cls ): metadata = ( field . metadata or {}) . get ( 'dataclasses_json' , {}) metadata = overrides [ field . name ] if metadata . mm_field is not None : schema [ field . name ] = metadata . mm_field else : type_ = field . type options = {} missing_key = 'missing' if infer_missing else 'default' if field . default is not MISSING : options [ missing_key ] = field . default elif field . default_factory is not MISSING : options [ missing_key ] = field . default_factory if options . get ( missing_key , ... ) is None : options [ 'allow_none' ] = True if _is_optional ( type_ ): options . setdefault ( missing_key , None ) options [ 'allow_none' ] = True if len ( type_ . __args__ ) == 2 : # Union[str, int, None] is optional too, but it has more than 1 typed field. type_ = type_ . __args__ [ 0 ] if metadata . letter_case is not None : options [ 'data_key' ] = metadata . letter_case ( field . name ) t = build_type ( type_ , options , mixin , field , cls ) # if type(t) is not fields.Field: # If we use `isinstance` we would return nothing. schema [ field . name ] = t return schema def build_schema ( cls : typing . Type [ A ], mixin , infer_missing , partial ) -> typing . Type [ SchemaType ]: Meta = type ( 'Meta' , (), { 'fields' : tuple ( field . name for field in dc_fields ( cls ) if field . name != 'dataclass_json_config' )}) @post_load def make_instance ( self , kvs , ** kwargs ): return _decode_dataclass ( cls , kvs , partial ) def dumps ( self , * args , ** kwargs ): if 'cls' not in kwargs : kwargs [ 'cls' ] = _ExtendedEncoder return Schema . dumps ( self , * args , ** kwargs ) schema_ = schema ( cls , mixin , infer_missing ) DataClassSchema : typing . Type [ SchemaType ] = type ( f '{cls.__name__.capitalize()}Schema' , ( Schema ,), { 'Meta' : Meta , f 'make_{cls.__name__.lower()}' : make_instance , 'dumps' : dumps , ** schema_ }) return DataClassSchema","title":"Module dataclasses_json.mm"},{"location":"reference/dataclasses_json/mm/#functions","text":"","title":"Functions"},{"location":"reference/dataclasses_json/mm/#build_schema","text":"def ( cls : Type [ ~ A ], mixin , infer_missing , partial ) -> Type [ dataclasses_json . mm . SchemaF [ ~ A ]] View Source def build_schema ( cls : typing . Type [ A ], mixin , infer_missing , partial ) -> typing . Type [ SchemaType ]: Meta = type ( ' Meta ' , () , { ' fields ' : tuple ( field . name for field in dc_fields ( cls ) if field . name != ' dataclass_json_config ' ) } ) @ post_load def make_instance ( self , kvs , ** kwargs ) : return _decode_dataclass ( cls , kvs , partial ) def dumps ( self , * args , ** kwargs ) : if ' cls ' not in kwargs : kwargs [ ' cls ' ] = _ExtendedEncoder return Schema . dumps ( self , * args , ** kwargs ) schema_ = schema ( cls , mixin , infer_missing ) DataClassSchema : typing . Type [ SchemaType ] = type ( f ' {cls.__name__.capitalize()}Schema ' , ( Schema , ) , { ' Meta ' : Meta , f ' make_{cls.__name__.lower()} ' : make_instance , ' dumps ' : dumps , ** schema_ } ) return DataClassSchema","title":"build_schema"},{"location":"reference/dataclasses_json/mm/#build_type","text":"def ( type_ , options , mixin , field , cls ) View Source def build_type ( type_ , options , mixin , field , cls ) : def inner ( type_ , options ) : while True : if not _is_new_type ( type_ ) : break type_ = type_ . __supertype__ if is_dataclass ( type_ ) : if _issubclass_safe ( type_ , mixin ) : options [ ' field_many ' ] = bool ( _is_supported_generic ( field . type ) and _is_collection ( field . type )) return fields . Nested ( type_ . schema () , ** options ) else : warnings . warn ( f \" Nested dataclass field {field.name} of type \" f \" {field.type} detected in \" f \" {cls.__name__} that is not an instance of \" f \" dataclass_json. Did you mean to recursively \" f \" serialize this field? If so, make sure to \" f \" augment {type_} with either the \" f \" `dataclass_json` decorator or mixin. \" ) return fields . Field ( ** options ) origin = getattr ( type_ , ' __origin__ ' , type_ ) args = [ inner ( a , {} ) for a in getattr ( type_ , ' __args__ ' , [] ) if a is not type ( None ) ] if origin in TYPES : return TYPES [ origin ] ( * args , ** options ) if _issubclass_safe ( origin , Enum ) : return EnumField ( enum = origin , by_value = True , * args , ** options ) if is_union_type ( type_ ) : union_types = [ a for a in getattr ( type_ , ' __args__ ' , [] ) if a is not type ( None ) ] union_desc = dict ( zip ( union_types , args )) return _UnionField ( union_desc , cls , field , ** options ) warnings . warn ( f \" Unknown type {type_} at {cls.__name__}.{field.name}: {field.type} \" f \" It's advised to pass the correct marshmallow type to `mm_field`. \" ) return fields . Field ( ** options ) return inner ( type_ , options )","title":"build_type"},{"location":"reference/dataclasses_json/mm/#schema","text":"def ( cls , mixin , infer_missing ) View Source def schema ( cls , mixin , infer_missing ) : schema = {} overrides = _user_overrides ( cls ) for field in dc_fields ( cls ) : metadata = ( field . metadata or {} ) . get ( ' dataclasses_json ' , {} ) metadata = overrides [ field . name ] if metadata . mm_field is not None : schema [ field . name ] = metadata . mm_field else : type_ = field . type options = {} missing_key = ' missing ' if infer_missing else ' default ' if field . default is not MISSING : options [ missing_key ] = field . default elif field . default_factory is not MISSING : options [ missing_key ] = field . default_factory if options . get ( missing_key , ... ) is None : options [ ' allow_none ' ] = True if _is_optional ( type_ ) : options . setdefault ( missing_key , None ) options [ ' allow_none ' ] = True if len ( type_ . __args__ ) == 2 : # Union [ str , int , None ] is optional too , but it has more than 1 typed field . type_ = type_ . __args__ [ 0 ] if metadata . letter_case is not None : options [ ' data_key ' ] = metadata . letter_case ( field . name ) t = build_type ( type_ , options , mixin , field , cls ) # if type ( t ) is not fields . Field : # If we use ` isinstance ` we would return nothing . schema [ field . name ] = t return schema","title":"schema"},{"location":"reference/dataclasses_json/mm/#classes","text":"","title":"Classes"},{"location":"reference/dataclasses_json/mm/#schemaf","text":"class ( * args , ** kwargs ) Lift Schema into a type constructor Raises exception because this class should not be inherited. This class is helper only.","title":"SchemaF"},{"location":"reference/dataclasses_json/mm/#ancestors-in-mro","text":"marshmallow.schema.Schema marshmallow.schema.BaseSchema marshmallow.base.SchemaABC typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/dataclasses_json/mm/#class-variables","text":"``` python3 opts ``` :","title":"Class variables"},{"location":"reference/dataclasses_json/mm/#methods","text":"##### dump ``` python3 def ( self , obj : Union [ List [ ~ A ], ~ A ], many : bool = None ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] ``` Serialize an object to native Python data types according to this Schema ' s fields. :param obj : The object to serialize . :param bool many : Whether to serialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :return : A dict of serialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the serialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if `` obj `` is invalid . .. versionchanged :: 3 . 0 . 0 rc9 Validation no longer occurs upon serialization . ??? example \" View Source \" def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass ##### dumps ``` python3 def ( self , obj : Union [ List [ ~ A ], ~ A ], many : bool = None , * args , ** kwargs ) -> str ``` Same as : meth :` dump `, except return a JSON - encoded string . :param obj : The object to serialize . :param bool many : Whether to serialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :return : A `` json `` string :rtype : str .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the serialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if `` obj `` is invalid . ??? example \" View Source \" def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass ##### load ``` python3 def ( self , data : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], many : bool = None , partial : bool = None , unknown : bool = None ) -> Union [ List [ ~ A ], ~ A ] ``` Deserialize a data structure to an object defined by this Schema ' s fields. :param dict data : The data to deserialize . :param bool many : Whether to deserialize ` data ` as a collection . If ` None `, the value for ` self . many ` is used . :param bool | tuple partial : Whether to ignore missing fields and not require any fields declared . Propagates down to `` Nested `` fields as well . If its value is an iterable , only missing fields listed in that iterable will be ignored . Use dot delimiters to specify nested fields . :param unknown : Whether to exclude , include , or raise an error for unknown fields in the data . Use ` EXCLUDE `, ` INCLUDE ` or ` RAISE `. If ` None `, the value for ` self . unknown ` is used . :return : A dict of deserialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the deserialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if invalid data are passed . ??? example \" View Source \" def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass ##### loads ``` python3 def ( self , json_data : Union [ str , bytes , bytearray ], many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> Union [ List [ ~ A ], ~ A ] ``` Same as : meth :` load `, except it takes a JSON string as input . :param str json_data : A JSON string of the data to deserialize . :param bool many : Whether to deserialize ` obj ` as a collection . If ` None `, the value for ` self . many ` is used . :param bool | tuple partial : Whether to ignore missing fields and not require any fields declared . Propagates down to `` Nested `` fields as well . If its value is an iterable , only missing fields listed in that iterable will be ignored . Use dot delimiters to specify nested fields . :param unknown : Whether to exclude , include , or raise an error for unknown fields in the data . Use ` EXCLUDE `, ` INCLUDE ` or ` RAISE `. If ` None `, the value for ` self . unknown ` is used . :return : A dict of deserialized data :rtype : dict .. versionadded :: 1 . 0 . 0 .. versionchanged :: 3 . 0 . 0 b7 This method returns the deserialized data rather than a `` ( data , errors ) `` duple . A : exc :` ValidationError < marshmallow . exceptions . ValidationError > ` is raised if invalid data are passed . ??? example \" View Source \" def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass View Source class SchemaF ( Schema , typing . Generic [ A ] ) : \"\"\"Lift Schema into a type constructor\"\"\" def __init__ ( self , * args , ** kwargs ) : \"\"\" Raises exception because this class should not be inherited. This class is helper only. \"\"\" super (). __init__ ( * args , ** kwargs ) raise NotImplementedError () @typing . overload def dump ( self , obj : typing . List [ A ] , many : bool = None ) -> typing . List [ TEncoded ] : pass @typing . overload def dump ( self , obj : A , many : bool = None ) -> TEncoded : pass def dump ( self , obj : TOneOrMulti , many : bool = None ) -> TOneOrMultiEncoded : pass @typing . overload def dumps ( self , obj : typing . List [ A ] , many : bool = None , * args , ** kwargs ) -> str : pass @typing . overload def dumps ( self , obj : A , many : bool = None , * args , ** kwargs ) -> str : pass def dumps ( self , obj : TOneOrMulti , many : bool = None , * args , ** kwargs ) -> str : pass @typing . overload def load ( self , data : typing . List [ TEncoded ] , many : bool = True , partial : bool = None , unknown : bool = None ) -> \\ typing . List [ A ] : pass @typing . overload def load ( self , data : TEncoded , many : None = None , partial : bool = None , unknown : bool = None ) -> A : pass def load ( self , data : TOneOrMultiEncoded , many : bool = None , partial : bool = None , unknown : bool = None ) -> TOneOrMulti : pass @typing . overload def loads ( self , json_data : JsonData , many : bool = True , partial : bool = None , unknown : bool = None , ** kwargs ) -> typing . List [ A ] : pass @typing . overload def loads ( self , json_data : JsonData , many : None = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> A : pass def loads ( self , json_data : JsonData , many : bool = None , partial : bool = None , unknown : bool = None , ** kwargs ) -> TOneOrMulti : pass","title":"Methods"},{"location":"reference/dataclasses_json/utils/","text":"Module dataclasses_json.utils View Source import inspect import sys from datetime import datetime , timezone from typing import Collection , Mapping , Optional def _get_type_cons ( type_ ): \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\" if sys . version_info . minor == 6 : try : cons = type_ . __extra__ except AttributeError : try : cons = type_ . __origin__ except AttributeError : cons = type_ else : cons = type_ if cons is None else cons else : try : cons = type_ . __origin__ if cons is None else cons except AttributeError : cons = type_ else : cons = type_ . __origin__ return cons def _get_type_origin ( type_ ): \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in the typing api\"\"\" try : origin = type_ . __origin__ except AttributeError : if sys . version_info . minor == 6 : try : origin = type_ . __extra__ except AttributeError : origin = type_ else : origin = type_ if origin is None else origin else : origin = type_ return origin def _hasargs ( type_ , * args ): try : res = all ( arg in type_ . __args__ for arg in args ) except AttributeError : return False else : return res def _isinstance_safe ( o , t ): try : result = isinstance ( o , t ) except Exception : return False else : return result def _issubclass_safe ( cls , classinfo ): try : return issubclass ( cls , classinfo ) except Exception : return ( _is_new_type_subclass_safe ( cls , classinfo ) if _is_new_type ( cls ) else False ) def _is_new_type_subclass_safe ( cls , classinfo ): super_type = getattr ( cls , \"__supertype__\" , None ) if super_type : return _is_new_type_subclass_safe ( super_type , classinfo ) try : return issubclass ( cls , classinfo ) except Exception : return False def _is_new_type ( type_ ): return inspect . isfunction ( type_ ) and hasattr ( type_ , \"__supertype__\" ) def _is_optional ( type_ ): return _issubclass_safe ( type_ , Optional ) or _hasargs ( type_ , type ( None )) def _is_mapping ( type_ ): return _issubclass_safe ( _get_type_origin ( type_ ), Mapping ) def _is_collection ( type_ ): return _issubclass_safe ( _get_type_origin ( type_ ), Collection ) def _is_nonstr_collection ( type_ ): return ( _issubclass_safe ( _get_type_origin ( type_ ), Collection ) and not _issubclass_safe ( type_ , str )) def _timestamp_to_dt_aware ( timestamp : float ): tz = datetime . now ( timezone . utc ) . astimezone () . tzinfo dt = datetime . fromtimestamp ( timestamp , tz = tz ) return dt","title":"Utils"},{"location":"reference/dataclasses_json/utils/#module-dataclasses_jsonutils","text":"View Source import inspect import sys from datetime import datetime , timezone from typing import Collection , Mapping , Optional def _get_type_cons ( type_ ): \"\"\"More spaghetti logic for 3.6 vs. 3.7\"\"\" if sys . version_info . minor == 6 : try : cons = type_ . __extra__ except AttributeError : try : cons = type_ . __origin__ except AttributeError : cons = type_ else : cons = type_ if cons is None else cons else : try : cons = type_ . __origin__ if cons is None else cons except AttributeError : cons = type_ else : cons = type_ . __origin__ return cons def _get_type_origin ( type_ ): \"\"\"Some spaghetti logic to accommodate differences between 3.6 and 3.7 in the typing api\"\"\" try : origin = type_ . __origin__ except AttributeError : if sys . version_info . minor == 6 : try : origin = type_ . __extra__ except AttributeError : origin = type_ else : origin = type_ if origin is None else origin else : origin = type_ return origin def _hasargs ( type_ , * args ): try : res = all ( arg in type_ . __args__ for arg in args ) except AttributeError : return False else : return res def _isinstance_safe ( o , t ): try : result = isinstance ( o , t ) except Exception : return False else : return result def _issubclass_safe ( cls , classinfo ): try : return issubclass ( cls , classinfo ) except Exception : return ( _is_new_type_subclass_safe ( cls , classinfo ) if _is_new_type ( cls ) else False ) def _is_new_type_subclass_safe ( cls , classinfo ): super_type = getattr ( cls , \"__supertype__\" , None ) if super_type : return _is_new_type_subclass_safe ( super_type , classinfo ) try : return issubclass ( cls , classinfo ) except Exception : return False def _is_new_type ( type_ ): return inspect . isfunction ( type_ ) and hasattr ( type_ , \"__supertype__\" ) def _is_optional ( type_ ): return _issubclass_safe ( type_ , Optional ) or _hasargs ( type_ , type ( None )) def _is_mapping ( type_ ): return _issubclass_safe ( _get_type_origin ( type_ ), Mapping ) def _is_collection ( type_ ): return _issubclass_safe ( _get_type_origin ( type_ ), Collection ) def _is_nonstr_collection ( type_ ): return ( _issubclass_safe ( _get_type_origin ( type_ ), Collection ) and not _issubclass_safe ( type_ , str )) def _timestamp_to_dt_aware ( timestamp : float ): tz = datetime . now ( timezone . utc ) . astimezone () . tzinfo dt = datetime . fromtimestamp ( timestamp , tz = tz ) return dt","title":"Module dataclasses_json.utils"},{"location":"venv/lib/python3.7/site-packages/Markdown-3.1.1.dist-info/LICENSE/","text":"Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version) All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the Python Markdown Project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE PYTHON MARKDOWN PROJECT ''AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL ANY CONTRIBUTORS TO THE PYTHON MARKDOWN PROJECT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"LICENSE"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/","text":"pdoc extracts documentation of: modules (including submodules), functions (including methods, properties, coroutines ...), classes, and variables (including globals, class variables, and instance variables). Documentation is extracted from live objects' docstrings using Python's __doc__ attribute 1 . Documentation for variables is found by examining objects' abstract syntax trees. What objects are documented? pdoc only extracts public API documentation. 2 All objects (modules, functions, classes, variables) are only considered public if their identifiers don't begin with an underscore ( _ ). 3 In addition, if a module defines __all__ , then only the identifiers contained in this list will be considered public. Otherwise, a module's global identifiers are considered public only if they don't begin with an underscore and are defined in this exact module (i.e. not imported from somewhere else). By transitivity, sub-objects of non-public objects (e.g. submodules of non-public modules, methods of non-public classes etc.) are not public and thus not documented. Where does pdoc get documentation from? In Python, objects like modules, functions, classes, and methods have a special attribute __doc__ which contains that object's documentation string ( docstring ). For example, the following code defines a function with a docstring and shows how to access its contents: >>> def test (): ... \"\"\"This is a docstring.\"\"\" ... pass ... >>> test . __doc__ 'This is a docstring.' It's pretty much the same with classes and modules. See PEP-257 for Python docstring conventions. These docstrings are set as descriptions for each module, class, function, and method listed in the documentation produced by pdoc . pdoc extends the standard use of docstrings in Python in two important ways: by allowing methods to inherit docstrings, and by introducing syntax for docstrings for variables. Docstrings inheritance pdoc considers methods' docstrings inherited from superclass methods', following the normal class inheritance patterns. Consider the following code example: >>> class A : ... def test ( self ) : ... \"\"\" Docstring for A. \"\"\" ... pass ... >>> class B ( A ) : ... def test ( self ) : ... pass ... >>> A . test . __doc__ ' Docstring for A. ' >>> B . test . __doc__ None In Python, the docstring for B.test doesn't exist, even though a docstring was defined for A.test . When pdoc generates documentation for the code such as above, it will automatically attach the docstring for A.test to B.test if B.test does not define its own docstring. In the default HTML template, such inherited docstrings are greyed out. Docstrings for variables Python by itself doesn't allow docstrings attached to variables . However, pdoc supports docstrings attached to module (or global) variables, class variables, and object instance variables; all in the same way as proposed in PEP-224 , with a docstring following the variable assignment. For example: module_variable = 1 \"\"\" Docstring for module_variable. \"\"\" class C : class_variable = 2 \"\"\" Docstring for class_variable. \"\"\" def __init__ ( self ) : self . variable = 3 \"\"\" Docstring for instance variable. \"\"\" While the resulting variables have no __doc__ attribute, pdoc compensates by reading the source code (when available) and parsing the syntax tree. By convention, variables defined in a class' __init__ method and attached to self are considered and documented as instance variables. Class and instance variables can also inherit docstrings . Overriding docstrings with __pdoc__ Docstrings for objects can be disabled or overridden with a special module-level dictionary __pdoc__ . The keys should be string identifiers within the scope of the module or, alternatively, fully-qualified reference names. E.g. for instance variable self.variable of class C , its module-level identifier is 'C.variable' . If __pdoc__[key] = False , then key (and its members) will be excluded from the documentation of the module. Alternatively, the values of __pdoc__ should be the overriding docstrings. This particular feature is useful when there's no feasible way of attaching a docstring to something. A good example of this is a namedtuple : __pdoc__ = {} Table = namedtuple ( ' Table ' , [ ' types ' , ' names ' , ' rows ' ] ) __pdoc__ [ ' Table.types ' ] = ' Types for each column in the table. ' __pdoc__ [ ' Table.names ' ] = ' The names of each column in the table. ' __pdoc__ [ ' Table.rows ' ] = ' Lists corresponding to each row in the table. ' pdoc will then show Table as a class with documentation for the types , names and rows members. .. note:: The assignments to __pdoc__ need to be placed where they'll be executed when the module is imported. For example, at the top level of a module or in the definition of a class. Supported docstring formats Currently, pure Markdown (with extensions ), numpydoc , and Google-style docstrings formats are supported, along with some reST directives . Additionally, if latex_math template config option is enabled, LaTeX math syntax is supported when placed between recognized delimiters : \\(...\\) for inline equations and \\[...\\] or $$...$$ for block equations. Note, you need to escape your backslashes in Python docstrings ( \\\\( , \\\\frac{}{} , ...) or, alternatively, use raw string literals . Supported reST directives The following reST directives should work: specific and generic admonitions , .. image:: or .. figure:: (without options), .. include:: , with support for the options: :start-line: , :end-line: , :start-after: and :end-before: . .. math:: .. versionadded:: .. versionchanged:: .. deprecated:: .. todo:: Linking to other identifiers In your documentation, you may refer to other identifiers in your modules. When exporting to HTML, linking is automatically done whenever you surround an identifier with backticks ( ` ). The identifier name must be fully qualified, for example `pdoc.Doc.docstring` is correct (and will link to pdoc.Doc.docstring ) while `Doc.docstring` is not . Command-line interface pdoc includes a feature-rich \"binary\" program for producing HTML and plain text documentation of your modules. For example, to produce HTML documentation of your whole package in subdirectory 'build' of the current directory, using the default HTML template, run: $ pdoc --html --output-dir build my_package To run a local HTTP server while developing your package or writing docstrings for it, run: $ pdoc --http : my_package To re-build documentation as part of your continuous integration (CI) best practice, i.e. ensuring all reference links are correct and up-to-date, make warnings error loudly by settings the environment variable PYTHONWARNINGS before running pdoc: $ export PYTHONWARNINGS = 'error::UserWarning' For brief usage instructions, type: $ pdoc --help Programmatic usage The main entry point is pdoc.Module which wraps a module object and recursively imports and wraps any submodules and their members. After all related modules are wrapped (related modules are those that share the same pdoc.Context ), you need to call pdoc.link_inheritance with the used Context instance to establish class inheritance links. Afterwards, you can use pdoc.Module.html and pdoc.Module.text methods to output documentation in the desired format. For example: import pdoc modules = [ 'a' , 'b' ] # Public submodules are auto-imported context = pdoc . Context () modules = [ pdoc . Module ( mod , context = context ) for mod in modules ] pdoc . link_inheritance ( context ) def recursive_htmls ( mod ): yield mod . name , mod . html () for submod in mod . submodules (): yield from recursive_htmls ( submod ) for mod in modules : for module_name , html in recursive_htmls ( mod ): ... # Process When documenting a single module, you might find functions pdoc.html and pdoc.text handy. For importing arbitrary modules/files, use pdoc.import_module . Alternatively, use the runnable script included with this package. Custom templates To override the built-in HTML/CSS and plain text templates, copy the relevant templates from pdoc/templates directory into a directory of your choosing and edit them. When you run pdoc command afterwards, pass the directory path as a parameter to the --template-dir switch. .. tip:: If you find you only need to apply minor alterations to the HTML template, see if you can do so by overriding just some of the following, placeholder sub-templates: * [ _config . mako_ ]: Basic template configuration , affects the way templates are rendered . * _head . mako_ : Included just before ` </ head > `. Best for adding resources and styles . * _logo . mako_ : Included at the very top of the navigation sidebar . Empty by default . * _credits . mako_ : Included in the footer , right before pdoc version string . See [ default template files ] for reference . .. tip:: You can also alter individual config.mako preferences using the --config command-line switch. If working with pdoc programmatically, prepend the directory with modified templates into the directories list of the pdoc.tpl_lookup object. Compatibility pdoc requires Python 3.5+. The last version to support Python 2.x is pdoc3 0.3.x . Contributing pdoc is on GitHub . Bug reports and pull requests are welcome. License pdoc is licensed under the terms of GNU AGPL-3.0 or later, meaning you can use it for any reasonable purpose and remain in complete ownership of all the documentation you produce, but you are also encouraged to make sure any upgrades to pdoc itself find their way back to the community. Documented modules are executed in order to provide __doc__ attributes. Any non-fenced global code in imported modules will affect the current environment. \u21a9 Here, public API refers to the API that is made available to your project end-users, not the public API e.g. of a private class that can be reasonably extended elsewhere by your project developers. \u21a9 Prefixing private, implementation-specific objects with an underscore is a common convention . \u21a9","title":"Documentation"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#what-objects-are-documented","text":"pdoc only extracts public API documentation. 2 All objects (modules, functions, classes, variables) are only considered public if their identifiers don't begin with an underscore ( _ ). 3 In addition, if a module defines __all__ , then only the identifiers contained in this list will be considered public. Otherwise, a module's global identifiers are considered public only if they don't begin with an underscore and are defined in this exact module (i.e. not imported from somewhere else). By transitivity, sub-objects of non-public objects (e.g. submodules of non-public modules, methods of non-public classes etc.) are not public and thus not documented.","title":"What objects are documented?"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#where-does-pdoc-get-documentation-from","text":"In Python, objects like modules, functions, classes, and methods have a special attribute __doc__ which contains that object's documentation string ( docstring ). For example, the following code defines a function with a docstring and shows how to access its contents: >>> def test (): ... \"\"\"This is a docstring.\"\"\" ... pass ... >>> test . __doc__ 'This is a docstring.' It's pretty much the same with classes and modules. See PEP-257 for Python docstring conventions. These docstrings are set as descriptions for each module, class, function, and method listed in the documentation produced by pdoc . pdoc extends the standard use of docstrings in Python in two important ways: by allowing methods to inherit docstrings, and by introducing syntax for docstrings for variables.","title":"Where does pdoc get documentation from?"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#docstrings-inheritance","text":"pdoc considers methods' docstrings inherited from superclass methods', following the normal class inheritance patterns. Consider the following code example: >>> class A : ... def test ( self ) : ... \"\"\" Docstring for A. \"\"\" ... pass ... >>> class B ( A ) : ... def test ( self ) : ... pass ... >>> A . test . __doc__ ' Docstring for A. ' >>> B . test . __doc__ None In Python, the docstring for B.test doesn't exist, even though a docstring was defined for A.test . When pdoc generates documentation for the code such as above, it will automatically attach the docstring for A.test to B.test if B.test does not define its own docstring. In the default HTML template, such inherited docstrings are greyed out.","title":"Docstrings inheritance"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#docstrings-for-variables","text":"Python by itself doesn't allow docstrings attached to variables . However, pdoc supports docstrings attached to module (or global) variables, class variables, and object instance variables; all in the same way as proposed in PEP-224 , with a docstring following the variable assignment. For example: module_variable = 1 \"\"\" Docstring for module_variable. \"\"\" class C : class_variable = 2 \"\"\" Docstring for class_variable. \"\"\" def __init__ ( self ) : self . variable = 3 \"\"\" Docstring for instance variable. \"\"\" While the resulting variables have no __doc__ attribute, pdoc compensates by reading the source code (when available) and parsing the syntax tree. By convention, variables defined in a class' __init__ method and attached to self are considered and documented as instance variables. Class and instance variables can also inherit docstrings .","title":"Docstrings for variables"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#overriding-docstrings-with-__pdoc__","text":"Docstrings for objects can be disabled or overridden with a special module-level dictionary __pdoc__ . The keys should be string identifiers within the scope of the module or, alternatively, fully-qualified reference names. E.g. for instance variable self.variable of class C , its module-level identifier is 'C.variable' . If __pdoc__[key] = False , then key (and its members) will be excluded from the documentation of the module. Alternatively, the values of __pdoc__ should be the overriding docstrings. This particular feature is useful when there's no feasible way of attaching a docstring to something. A good example of this is a namedtuple : __pdoc__ = {} Table = namedtuple ( ' Table ' , [ ' types ' , ' names ' , ' rows ' ] ) __pdoc__ [ ' Table.types ' ] = ' Types for each column in the table. ' __pdoc__ [ ' Table.names ' ] = ' The names of each column in the table. ' __pdoc__ [ ' Table.rows ' ] = ' Lists corresponding to each row in the table. ' pdoc will then show Table as a class with documentation for the types , names and rows members. .. note:: The assignments to __pdoc__ need to be placed where they'll be executed when the module is imported. For example, at the top level of a module or in the definition of a class.","title":"Overriding docstrings with __pdoc__"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#supported-docstring-formats","text":"Currently, pure Markdown (with extensions ), numpydoc , and Google-style docstrings formats are supported, along with some reST directives . Additionally, if latex_math template config option is enabled, LaTeX math syntax is supported when placed between recognized delimiters : \\(...\\) for inline equations and \\[...\\] or $$...$$ for block equations. Note, you need to escape your backslashes in Python docstrings ( \\\\( , \\\\frac{}{} , ...) or, alternatively, use raw string literals .","title":"Supported docstring formats"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#supported-rest-directives","text":"The following reST directives should work: specific and generic admonitions , .. image:: or .. figure:: (without options), .. include:: , with support for the options: :start-line: , :end-line: , :start-after: and :end-before: . .. math:: .. versionadded:: .. versionchanged:: .. deprecated:: .. todo::","title":"Supported reST directives"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#linking-to-other-identifiers","text":"In your documentation, you may refer to other identifiers in your modules. When exporting to HTML, linking is automatically done whenever you surround an identifier with backticks ( ` ). The identifier name must be fully qualified, for example `pdoc.Doc.docstring` is correct (and will link to pdoc.Doc.docstring ) while `Doc.docstring` is not .","title":"Linking to other identifiers"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#command-line-interface","text":"pdoc includes a feature-rich \"binary\" program for producing HTML and plain text documentation of your modules. For example, to produce HTML documentation of your whole package in subdirectory 'build' of the current directory, using the default HTML template, run: $ pdoc --html --output-dir build my_package To run a local HTTP server while developing your package or writing docstrings for it, run: $ pdoc --http : my_package To re-build documentation as part of your continuous integration (CI) best practice, i.e. ensuring all reference links are correct and up-to-date, make warnings error loudly by settings the environment variable PYTHONWARNINGS before running pdoc: $ export PYTHONWARNINGS = 'error::UserWarning' For brief usage instructions, type: $ pdoc --help","title":"Command-line interface"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#programmatic-usage","text":"The main entry point is pdoc.Module which wraps a module object and recursively imports and wraps any submodules and their members. After all related modules are wrapped (related modules are those that share the same pdoc.Context ), you need to call pdoc.link_inheritance with the used Context instance to establish class inheritance links. Afterwards, you can use pdoc.Module.html and pdoc.Module.text methods to output documentation in the desired format. For example: import pdoc modules = [ 'a' , 'b' ] # Public submodules are auto-imported context = pdoc . Context () modules = [ pdoc . Module ( mod , context = context ) for mod in modules ] pdoc . link_inheritance ( context ) def recursive_htmls ( mod ): yield mod . name , mod . html () for submod in mod . submodules (): yield from recursive_htmls ( submod ) for mod in modules : for module_name , html in recursive_htmls ( mod ): ... # Process When documenting a single module, you might find functions pdoc.html and pdoc.text handy. For importing arbitrary modules/files, use pdoc.import_module . Alternatively, use the runnable script included with this package.","title":"Programmatic usage"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#custom-templates","text":"To override the built-in HTML/CSS and plain text templates, copy the relevant templates from pdoc/templates directory into a directory of your choosing and edit them. When you run pdoc command afterwards, pass the directory path as a parameter to the --template-dir switch. .. tip:: If you find you only need to apply minor alterations to the HTML template, see if you can do so by overriding just some of the following, placeholder sub-templates: * [ _config . mako_ ]: Basic template configuration , affects the way templates are rendered . * _head . mako_ : Included just before ` </ head > `. Best for adding resources and styles . * _logo . mako_ : Included at the very top of the navigation sidebar . Empty by default . * _credits . mako_ : Included in the footer , right before pdoc version string . See [ default template files ] for reference . .. tip:: You can also alter individual config.mako preferences using the --config command-line switch. If working with pdoc programmatically, prepend the directory with modified templates into the directories list of the pdoc.tpl_lookup object.","title":"Custom templates"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#compatibility","text":"pdoc requires Python 3.5+. The last version to support Python 2.x is pdoc3 0.3.x .","title":"Compatibility"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#contributing","text":"pdoc is on GitHub . Bug reports and pull requests are welcome.","title":"Contributing"},{"location":"venv/lib/python3.7/site-packages/pdoc/documentation/#license","text":"pdoc is licensed under the terms of GNU AGPL-3.0 or later, meaning you can use it for any reasonable purpose and remain in complete ownership of all the documentation you produce, but you are also encouraged to make sure any upgrades to pdoc itself find their way back to the community. Documented modules are executed in order to provide __doc__ attributes. Any non-fenced global code in imported modules will affect the current environment. \u21a9 Here, public API refers to the API that is made available to your project end-users, not the public API e.g. of a private class that can be reasonably extended elsewhere by your project developers. \u21a9 Prefixing private, implementation-specific objects with an underscore is a common convention . \u21a9","title":"License"},{"location":"venv/lib/python3.7/site-packages/pymdown_extensions-6.0.dist-info/LICENSE/","text":"The MIT License (MIT) (Except where stated below) Copyright (c) 2014 - 2018 Isaac Muse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. superfences.py is derived from Python Markdown's fenced_code extension. Fenced Code Extension for Python Markdown ========================================= This extension adds Fenced Code Blocks to Python-Markdown. See https://python-markdown.github.io/extensions/fenced_code_blocks/ for documentation. Original code Copyright 2007-2008 Waylan Limberg . All changes Copyright 2008-2014 The Python Markdown Project License: BSD inlinehilite.py is derived from Python Markdown's codehilite extension. CodeHilite Extension for Python-Markdown ======================================== Adds code/syntax highlighting to standard Python-Markdown code blocks. See https://python-markdown.github.io/extensions/code_hilite/ for documentation. Original code Copyright 2006-2008 Waylan Limberg . All changes Copyright 2008-2014 The Python Markdown Project License: BSD extrarawhtml.py is a literal copy and paste from Python Markdown's extra extension. It basically splits out the raw html markdown parsing into a seprate extension that can be used even if it is not desired to use all of 'extra'. Python-Markdown Extra Extension =============================== See https://python-markdown.github.io/extensions/extra/ for documentation. Copyright The Python Markdown Project License: BSD gemoji_db.py is generated from Gemoji's source code: https://github.com/github/gemoji. Copyright (c) 2013 GitHub, Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. emoji1_db.py is generated from EmojiOne's source code: https://github.com/Ranks/emojione EmojiOne Non-Artwork Applies to the Javascript, JSON, PHP, CSS, HTML files, and everything else not covered under the artwork license above. License: MIT Complete Legal Terms: http://opensource.org/licenses/MIT","title":"LICENSE"}]}